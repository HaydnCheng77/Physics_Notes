\documentclass[english,a4paper,12pt]{report}
\usepackage{mypackage}

\title{Maths (Miscellaneous)}

\author{Haydn Cheng}

\date{\today}

\begin{document}
\maketitle
\tableofcontents

\chapter{Complex Numbers}

A complex number is most conveniently thought of as a vector with the \(x\)-axis replaced by the real axis with unit vector \(1\) and the \(y\)-axis replaced by the imaginary axis with unit vector \(i = \sqrt{-1} \). An arbitrary vector can then be represented in polar form or Cartesian form

\begin{equation}
	z = re^{i \theta } = x + iy, 
\end{equation}

where \( r = \abs{z} = \sqrt{x^2 + y^2} \text { and } \theta = \arg (z)\) is the angle between the complex number (vector) and the positive \(x\)-axis. If the complex number (vector) lies in the first quadrant, then \(\arg (z) = \arctan {\left(y /x \right)} \), otherwise, \(\pi \text { or } 2\pi \) must be added, since the domain of the arctan function is only from \(- \pi /2\) to \(\pi  /2 \). \(2\pi \) can also be added to any argument, but usually we use the pricipal value of the argument, \textit{i.e.,} \(\arg (z) \in [-\pi , \pi ) \text { or } [0,2\pi )\).   

In particular, if \(r=1\), then it is easy to show that 

\begin{equation}
	e^{i \theta } = \cos \theta + i\sin \theta ,
\end{equation}

since we simply decompose a complex number (vector) into its components. This equation can also be verified using the Taylors' expansion of the exponential, sine and cosine function.

Raising both sides of the above equation to the power \(n\), we have

\begin{equation}
	(\cos \theta + i \sin \theta )^{n} = e^{i n \theta } = \cos (n \theta ) + i \sin (n \theta ).
\end{equation}

Using the polar form, it is trivial to derive the relations

\begin{equation}
	\abs{z_1 z_2 } = \abs{z_1 }\abs{z_2 } \text { and } \arg (z_1 z_2 ) = \arg (z_1 ) + \arg (z_2 ).
\end{equation}

Hence multiplying two complex numbers (vectors) together can be thought separately as two process. First, the modulus (length) of the new complex number (vector) is the product of the modulus (length) of the two complex numbers (vectors). Second, the argument of the new complex number (vector) is the sum of the arguments of the two complex numbers (vectors).

The complex conjugate of a complex number is defined as 

\begin{equation}
	z^* = x-iy = re^{-i \theta }.
\end{equation}

For complex number with a more complicated form, we simply replace all \(i\) by \(-i\). This follows from the fact that \((z_1 + z_2 )^* = z_1 ^* + z_2 ^*\) and \((z_1 z_2 )^* = z_1 ^* z_2 ^*\).  

Geometrically, we flip the imaginary axis (\(y\)-axis) upside down. Therefore, \(z \text { and } z^*\) are two complex numbers (vectors) reflected about the real axis (\(x\)-axis). Thus we have

\begin{equation}
	z z^* = \abs{z}^2 
\end{equation}

When extend to complex numbers, the natural logarithm is always multivalued, this is because 

\begin{equation}
    \ln (z) = \ln (\abs{z}e^{(i \theta )}  ) = \ln (\abs{z} ) + i \theta .
\end{equation}

The hyperbolic trigonometric formulas are largely reminiscent of the ordinary trigonometric formulas, with a few exceptions:

\begin{equation}
    \cosh^2(x) - \sinh^2(x) = 1, \quad 1 - \tanh^2(x) = \sech^2(x), \quad \text{and} \cosh^2(x) + \sinh^2(x) = \cosh(2x).
\end{equation}




\example{Inverse Trigonometric Functions of Complex Inputs.}
{Express \(\tan ^{-1} {(\sqrt{3}i )} \) in the form \(a+bi\). }
{Define \(z \equiv \tan ^{-1} {(\sqrt{3}i)} \text { and } \alpha \equiv e^{iz} \), then 
\begin{equation}
    \begin{aligned}
        \sqrt{3}i &= \tan (z) = \frac{e^{iz}-e^{-iz}  }{i(e^{iz} +e^{-iz} )} \implies \alpha = \pm \sqrt{\frac{\sqrt{3}-1 }{\sqrt{3}+1 } }i \\
        z &= \frac{\ln \alpha }{i} = -i\left( \frac{1}{2}\ln \left(\frac{\sqrt{3}-1 }{\sqrt{3}+1 }\right) + \ln (\pm i)  \right) \\ &= -i\ln (e^{\frac{1}{2}(\pm i\pi (1+2k)) } ) - \frac{1}{2}\ln \left( \frac{\sqrt{3}-1 }{\sqrt{3} +1}  \right) i \\ &= \frac{\pi }{2}(1+2k) - \frac{1}{2}\ln \left( \frac{\sqrt{3}-1 }{\sqrt{3}+1 } \right)i.      
    \end{aligned}
\end{equation}~

 } 

 \example{Inverse Hyperbolic Functions of Complex Inputs.}
 {Express \(\sinh^{-1}(-1) \) in the form \(a+bi\).   }
 {Define \(z \equiv \sinh^{-1} (\sqrt{3}i) \text { and } \alpha \equiv e^{z} \), then 
 \begin{equation}
        \alpha = \sqrt{2}-1   \implies  z = \ln (\abs{\sqrt{2}-1 })  + 2\pi ni = \frac{1}{2} \ln (3 )+2\pi ni.
 \end{equation}

 Note that we have discarded the negative root of the quadratic equation, this is because for real \(z\), \(\alpha = e^{z} \) must be positive.   

 Note also that the absolute sign means the norm (length) of the complex number (vector), but not just fixing the positive and negative signs.
 
 } 
 


\example{Plot on the Complex Plane.}
{Plot the equation \( \arg \left((z-4)/(z-1)\right)=\frac{3\pi }{2} \) on the complex plane. }
{Writing \(z = x+iy\), we have 

\begin{equation}
    \frac{z-4}{z-1} = \frac{(x-4)(x-1)+y^2}{(x-1)^2+y^2} + i \frac{3y}{(x-1)^2+y^2}.  
\end{equation}

The real part must be zero and the imaginary part must be negative for the argument to be \(3\pi /2\), so we get \((x-4)(x-1)+y^2=0 \implies (x-\frac{5}{2} )^2+y^2 = 0 \text { and } y<0\). Note we cannot use the formula \(\arg (z) =  \arctan {\left(y /x\right) } \), since the complex number does not lie in the first quadrant.   
} 

To find the cosine or sine of some multiples of angle (\textit{e.g.,} \(\sin (6\theta )\) ), the simpliest way is always expand \((\cos \theta +i\sin \theta )^6 = \cos (6\theta ) + i\sin (6\theta )\) and compare the imaginary (or real) part. 

To find some power of cosine or sine of angle (\textit{e.g.,} \((\sin ^6(\theta ))\) ) in terms of single power of cosine or sine, the sipmliest way is always write \(\sin \theta = e^{i \theta }-e^{-i \theta }  /2i \), and expand. 

The fundamental theorem of algebra states that a \(n^{\text{th}}\) order polynomial equation will have exactly \(n\) roots. For example, the equation \(x^{8} = 16\) has eight roots \(x = \sqrt[8]{16} = (16e^{2\pi n})^{\frac{1}{8} } = \sqrt{2}e^{\frac{\pi n}{4} }\), where \(n = (0,\ldots 7) \text { or } (1,\ldots 8)\).   

The sum and product of roots of the polynomial equation

\begin{equation}
    a_0 + a_1 z + a_2 z ^2 + \cdots + a_{n}z^2 = a_{n}(z - z_1 )(z-z_2 )\cdots (z-z_{n} ) = 0   
\end{equation}

can be extracted to be 

\begin{equation}
    \sum_{i=1}^{n} z_{i} = - \frac{a_{n-1} }{a_{n} } ~\text { and }~ \prod_{i=1}^{n} z_{i} = (-1)^{n} \left(\frac{a_{0} }{a_{n} }\right).     
\end{equation}



\chapter{Fourier Series and Integral Transforms}

\section{Fourier Series}

The Fourier series expansion of the function \(f(x)\) with period \(L\) is conventionally written as 

\begin{equation}
    f(x) = \frac{a_0 }{2} + \sum_{r=1}^{\infty} \left( a_{r} \cos \left( \frac{2\pi rx}{L}  \right) + b_{r} \sin \left( \frac{2\pi rx}{L}  \right)  \right). 
\end{equation}

All the terms of a Fourier series are mutually orthogonal, \textit{i.e.,} 

\begin{equation}
    \begin{aligned}
        \int_{x_0}^{x_0+L} \sin\left(\frac{2\pi r x}{L}\right) \cos\left(\frac{2\pi p x}{L}\right) dx &= 0 \quad \text{for all } r \text{ and } p, \\
        \int_{x_0}^{x_0+L} \cos\left(\frac{2\pi r x}{L}\right) \cos\left(\frac{2\pi p x}{L}\right) dx &=
\begin{cases} 
    0 & \text{for } r = p = 0, \\
    \frac{L}{2} & \text{for } r = p > 0, \\
    0 & \text{for } r \neq p,
\end{cases} \\
\int_{x_0}^{x_0+L} \sin\left(\frac{2\pi r x}{L}\right) \sin\left(\frac{2\pi p x}{L}\right) dx &=
\begin{cases} 
    0 & \text{for } r = p = 0, \\
    \frac{L}{2} & \text{for } r = p > 0, \\
    0 & \text{for } r \neq p.
\end{cases}
    \end{aligned}
\end{equation}

Thus, the Fourier coefficient of a certain cosine or sine function with a particular \(r\) can be found by multiplying \(f(x)\) same cosine and sine function and then integrate, which would yield the result \(a_{r}L/2  \text { or } b_{r}L/2  \). Thus the Fourier coefficients are given by 

\begin{equation}
    a_{r} = \frac{2}{L} \int_{x_0}^{x_0 + L} f(x) \cos \left( \frac{2\pi rx}{L}  \right) dx \text { and } b_{r} = \frac{2}{L} \int_{x_0 }^{x_0 + L} f(x) \sin \left( \frac{2\pi rx}{L}  \right) dx.   
\end{equation}

Any arbitrary function \(f(x)\) can be decomposed into the sum of an even and an odd function, since

\begin{equation}
    f(x) = \frac{1}{2}(f(x) + f(-x)) + \frac{1}{2} (f(x) - f(-x)) = f_{\text{odd} }(x) + f_{\text{even} }(x).
\end{equation}

Comparing the above equation with the Fourier expansion of \(f(x)\), we see that all the cosines terms sum up to \(f_{\text{even}}(x)\) and all the sines terms sum up to \(f_{\text{odd} }(x)\). 

Therefore, if the function \(f(x)\) is even, the all the coefficients of the sines terms are zero and vice versa. 

For a function \(f(x)\) that is symmetric (even or odd) about \(L /4 \), \textit{i.e.,} \(f(L /4-x ) = \pm f(x - \frac{L}{4} )\), we make the substitution \(s = x-L /4 \) and we have 

\begin{equation}
    \begin{aligned} 
    b_{r} &= \frac{2}{L} \int_{x_0 }^{x_0 + L} f(s) \sin \left( \frac{2 \pi rs}{L} + \frac{\pi r}{2} \right) ds \\ &= \frac{2}{L} \int_{x_0 }^{x_0 + L} f(s) \left( \sin \left( \frac{2 \pi  rs}{L}  \right) \cos \left( \frac{\pi r}{2}  \right) + \cos \left( \frac{2 \pi rs}{L}  \right) \sin \left( \frac{\pi r}{2} \right)\right) ds.        
    \end{aligned} 
\end{equation}

If \(r\) is even then the second term in the integrand vanishes. The integral becomes the normal Fourier coefficient, but the dummy variable is now \(s\) instead of \(x\). If \(f(s)\) is also even, then we can conclude that the integral is zero, as for all even function the fourier coefficient \(b_{r} = 0\). By similar means, we can conclude that 

\begin{enumerate}
    \item If \(f(x)\) is even about \(\frac{L}{4} \) then \(a_{2r+1} = 0 \text { and } b_{2r} = 0 \).
    \item If \(f(x)\) is odd about \(\frac{L}{4} \) then \(a_{2r} = 0 \text { and } b_{2r+1} = 0 \).      
\end{enumerate}

For non-periodic function in a finite range we can simply extend the function to make it periodic. However, if the periodic function has discontinuity then the value obtained from the Fourier series will converge to a value halfway between the upper and lower values.

Leveraging the Euler's identity \(e^{irx} = \cos (rx) + i\sin (rx) \), the Fourier series can be written in a more compact form

\begin{equation}
    f(x) = \sum_{r=-\infty}^{+\infty} c_{r} e^{\left( \frac{2\pi i rx}{L}  \right)}dx,  
\end{equation}

where the coefficients are 

\begin{equation}
    c_{r} = \frac{1}{L} \int_{x_0 }^{x_0 + L} f(x)e^{\left( -\frac{2\pi irx}{L}  \right)}dx,     
\end{equation}

since

\begin{equation}
    \int_{x_0 }^{x_0 + L} e^{\left( -\frac{2\pi ipx}{L}\right)} e^{\left( \frac{2\pi irx}{L} \right)} dx = \begin{cases} L & \text{for } r=p, \\ 0 & \text{for } r\neq p.\end{cases}   
\end{equation}

To relate \(a_{r},b_{r} \text { and } c_{r}  \), we have

\begin{equation}
    c_{r} = \frac{1}{2} (a_{r} -ib_{r}  ) \text { and } c_{-r} = \frac{1}{2}(a_{r} + ib_{r}  ).   
\end{equation}

If \(f(x)\) is real then \(c_{-r} = c_{r}^*  \).

Parseval's theorem states that 

\begin{equation}
    \frac{1}{L} \int_{x_0 }^{x_0 + L} \abs{f(x)}^2 dx = \sum_{r=-\infty}^{+\infty} \abs{c_{r} }^2 = \left(\frac{a_0 }{2} \right)^2 + \frac{1}{2} \sum_{r=1}^{\infty} (a_{r}^2 + b_{r}^2).         
\end{equation}

To prove this theorem, consider two functions \(f(x) \text { and } g(x)\) with Fourier series 

\begin{equation}
    f(x) = \sum_{r=-\infty}^{+\infty} c_{r} e^{\left( \frac{2\pi irx}{L}  \right)} ~\text { and }~ g(x) = \sum_{p=-\infty}^{+\infty} d_{p} e^{\left( \frac{2\pi ipx}{L}  \right)}.      
\end{equation}

Now consider 

\begin{equation}
    \begin{aligned} 
    \frac{1}{L} \int_{x_0 }^{x_0 + L} f(x)g^*(x)dx &= \sum_{r=-\infty}^{+\infty} c_{r} \frac{1}{L} \int_{x_0 }^{x_0 + L} g^*(x) e^{\frac{2\pi irx}{L} }dx \\ &= \sum_{r=-\infty}^{+\infty} c_{r} \left( \frac{1}{L} \int_{x_0 }^{x_0 + L} g(x) e^{\left( \frac{-2\pi irx}{L}  \right)}   \right)^* = \sum_{r=-\infty}^{+\infty} c_{r} \gamma _{r}^*.            
    \end{aligned} 
\end{equation}

\section{Fourier Transforms}

Replacing the general variable \(x\) with time \(t\), we can express any time-varing function with period \(T\) as 

\begin{equation}
    f(t) = \sum_{r=-\infty}^{+\infty} c_{r} e^{\left( \frac{2\pi irt}{T}  \right)} = \sum_{r=-\infty}^{+\infty} c_{r} e^{i\omega _{r} t }, ~~~ c_{r} = \frac{1}{T} \int_{-\frac{T}{2} }^{\frac{T}{2} } f(t) e^{-\frac{2\pi irt}{T} }dt = \frac{\Delta \omega }{2\pi } \int_{-\frac{T}{2} }^{\frac{T}{2} } f(t) e^{-i \omega _{r} t} dt    ,        
\end{equation}

where we have defined 

\begin{equation}
    \omega _{r} \equiv \frac{2\pi r}{T} \implies \Delta \omega = \frac{2\pi }{T}.   
\end{equation}

For function with no periodicity, we have \(T \to \infty\) and \(\Delta \omega = 2\pi /T \to 0 \). Thus we have

\begin{equation}
    \begin{aligned} 
    f(t) &= \sum_{r=-\infty}^{+\infty} \frac{\Delta \omega }{2\pi } \left( \int_{-\infty}^{\infty } f(t) e^{- i \omega _{r} t} dt \right) e^{i \omega _{r} t} \\
    &= \frac{1}{2\pi } \int_{-\infty}^{+\infty} e^{i \omega t}  \left( \int_{-\infty}^{\infty } f(t) e^{- i \omega t} dt \right) d \omega 
    \end{aligned} 
\end{equation}

The integral in the bracket (times \(1 /\sqrt{2\pi }  \)) is defined to be the Fourier transform of \(f(t)\), denoted by 

\begin{equation}
    \tilde{f}(\omega ) = \frac{1}{\sqrt{2\pi } } \int_{-\infty}^{+\infty} f(t) e^{-i \omega t}dt.     
\end{equation}

The whole integral is the inverse Fourier transform, denoted by 

\begin{equation}
    f(t) = \frac{1}{\sqrt{2\pi } } \int_{-\infty}^{+\infty} \tilde{f}(\omega )e^{i \omega t} d \omega .     
\end{equation}

\example{The Uncertainty Principle}
{Find the Fourier transform of the normalized Gaussian distribution 

\begin{equation}
    f(t) = \frac{1}{\tau \sqrt{2\pi } } e^{-\frac{t^2}{2\tau ^2} }.
\end{equation}~
}
{The Fourier transform of \(f(t)\) is given by 

\begin{equation}
    \begin{aligned} 
    \tilde{f}(\omega ) &= \frac{1}{\sqrt{2\pi } } \int_{-\infty}^{+\infty} \frac{1}{\tau \sqrt{2\pi } } e^{-\frac{t^2}{2\tau ^2} } e^{-i \omega t}dt \\
    &= \frac{1}{2\pi  } \int_{-\infty}^{+\infty}  e^{-\frac{1}{2\tau ^2} (t^2 + 2\tau ^2i \omega t + (\tau ^2i \omega )^2 - (\tau ^2 i \omega )^2) } dt \\
    &= \frac{e^{-\frac{\tau ^2\omega ^2}{2} } }{2\pi  }  \int_{-\infty}^{+\infty} e^{-\frac{(t-i\tau ^2\omega ^2)^2}{2\tau ^2} }dt = \frac{e^{-\frac{\tau ^2\omega ^2}{2} } }{2\pi  },
    \end{aligned}         
\end{equation}

which is another Gaussian distribution with a root mean square deviation of \(\frac{1}{\tau } \). Thus the standard deviation in \(t \text { and } \tau \) is inversely related, \textit{i.e.,} \(\sigma _{t}\sigma _{\omega }=1  \), independent of the value of \(\tau \). 

In quantum mechanics, \(f(t) \text { or }  f(x)\) represents a wave function evoluting in time or space and the probability of finding the particle at position \(x\) at time \(t\) is given by \(\abs{f(x)}^2 \text { or } \abs{f(t)}^2  \), which is also a Gaussian but with the standard deviation multiplied by a factor of \(\frac{1}{\sqrt{2} } \). 

Similarly, the probability distribution in terms of frequency and wave vector is given by \(\abs{f(\omega )}^2 \text { or } \abs{f(k)}^2 \). 

Therefore, since \(p = \hbar k \text { and } E = \hbar \omega \), we have 

\begin{equation}
    \Delta E \Delta t = \frac{\hbar }{2} ~\text { and }~ \Delta p\Delta x = \frac{\hbar }{2}.  
\end{equation}


} 


\chapter{Statistics}
		
\section{Standard Deviation}

For a set of data which contains of \(N\) distinct values \(j_1, j_2, \ldots , j_i, \ldots , j_N\) and each with frequency \(f_1, f_2, \ldots , f_i, \ldots f_N\), we can define \( P_i = f_i / \sum_{i=1}^{N} f_i\) as the probability of selecting the data \(j_i\), then the average value of any function \(g(j)\) can be written as 
	
\begin{equation}
	 \avg{g(j)}  = \frac{f_1 g(j_1) + f_2 g(j_2) + \cdots + f_N g(j_N)}{f_1 + f_2 + \cdots + f_N} = \sum_{i=0}^{N} P_j g(j_i). \label{prob} 
\end{equation}
	
To measure the dispersion of the set of data, the most intuitive way is to calculate the average of difference between each data and the mean

\begin{equation} 
	\sigma' = \frac{f_1(j_1 - \avg{ j }) + f_2(j_2 - \avg{ j }) + \cdots + f_N(j_N - \avg{ j })}{f_1 + f_2 + \cdots + f_N}. \label{abc} 
\end{equation}

However, \(\sigma'\) always equals to zero since \( \sum_{i=1}^{N} f_i j_i = \avg{ j } \sum_{i=1}^{N} f_i \). So, either we take the absolute value of each term or we square each term in \cref{abc} such that the result is non trivial. We adopt the latter choice since the former is tedious. We introduce the quantity standard deviation \(\sigma\) defined by

\begin{equation} 
	\sigma^2 = \frac{f_1(j_1 - \avg{ j })^2 + f_2(j_2 - \avg{ j })^2 + \cdots + f_N(j_N - \avg{ j })^2}{f_1 + f_2 + \cdots + f_N} = \sum_{i=0}^{N} P_j (j_i - \avg{ j })^2. \label{sigma} 
\end{equation}

Note that \(\sigma\) in \cref{sigma} is squared so that the standard deviation has the same dimension as \(j\).

By expanding the bracket in \cref{sigma} and applying \cref{prob}, we have

\begin{equation} 
	\begin{aligned} 
		\sigma^2 &= \sum_{i=1}^{N} P_i (j_i^2 - 2j_i\avg{ j } + \avg{ j }^2) \\ &= \sum_{i=1}^{N} (j_i)^2 P_i - 2 \avg{ j } \sum_{i=1}^{N} (j_i) P_i + \avg{ j }^2 \sum_{i=1}^{N} P_i \\ &= \avg{ j^2 } - 2\avg{ j } \avg{ j } + \avg{ j }^2 = \avg{ j^2 } - \avg{ j }^2. 
	\end{aligned} 
\end{equation}

Therefore, we have the useful identity

\begin{equation} 
	\sigma = \sqrt{\avg{ j^2 } - \avg{ j }^2}. 
\end{equation}
	
\section{Probability Density Function}
	
When we consider continuous variable, the probability of obtaining a certain value becomes meaningless now as there are infinite choices now so we define the probability density function \(\rho(x)\) such that the probability of obtaining a value between \(x\) and \(x + dx\) equals to \(\rho(x)dx\) (its discrete counterpart being \(P_{i}\)). Therefore, just like the previous section, we have the relations
	
\begin{equation} 
	\int_{-\infty}^{+\infty} \rho(x)dx = 1 \text { and } 	\avg{ f(x) } = \int_{-\infty}^{+\infty} f(x)\rho(x) dx .
\end{equation}

	
\example{Free Falling of an Object.}
{Consider an object being released at height \(h\). Find the average distance \(\avg{x}\) from the point of release if a random instant is chosen.}
{Let \(\rho(x)dx\) be the probability of a random instant being located between \(x\) and \(x + dx\) which is equals to \(\frac{dt}{T}\). Since \(v = \dv{x}{t} = gt\) and \(T = \sqrt{\frac{2h}{g}}\), so we have \(\rho(x) = \frac{1}{2\sqrt{hx}}\).\(\avg{x}\) can then be obtained from straightforward integration 
\begin{equation} 
	\avg{x} = \int_{0}^{h} \frac{x}{2\sqrt{hx}} dx = \frac{h}{3}.
\end{equation}}

We can define a joint probability distribution \(\rho _{x,y}(x,y) \), where \(\rho _{x,y}(x,y)dxdy \) is the probability for the variable \(x\) to obtain a value between \(x \text { and } x+dx\), and for the variable \(y\) to obtain a value between \(y \text { and } y+dy\). 

The marginal distribution \(\rho _{x}(x) \) regardless of the value of \(y\) is obtained by integrating the joint probability density function over all allowed values of \(y\) to get \(\rho _{x}(x) = \int_{-\infty}^{+\infty}  \rho _{x,y}(x,y)dy\).

The conditional distribution \(\rho _{x} (x)\) given \(y = y_0 \) is obtained by \(\rho _{x} (x) = \frac{\rho _{x,y} (x,y_0 )}{\rho _{y}(y_0 ) } \), where the denominator serving a renormalization purpose.

\example{Joint Probability Density Function.}
{Alice and Bob agree to meet for lunch at some time between noon and 1 pm. They are both willing to wait up to 10 min for the other to arrive, but will leave otherwise. Find the probability that they meet for lunch, if
\begin{enumerate}[itemsep=10pt]
	\item their arrival times are independent and uniformly distributed, or
	\item their arrival times are independent, but while Alice's probability of arrival is uniform, Bob is quadractically more likely to arrive later in the hour.    
\end{enumerate}~}
{\begin{enumerate}
    \item For the first case, we have \(\rho _{x} = \rho _{y} = C\). Normalizing gives \(C = \frac{1}{60 \text{mins} } \). So the joint probability density function is \(\rho _{x,y} = \frac{1}{3600 \text{mins} }  \). We then have to integrate the joint probability density function over the region in which \(\abs{y-x} \le 10 \) 
    
    \begin{equation}
        P(\abs{y-x}\le 10 ) = \int_{\abs{y-x} \le 10}^{}\rho _{x,y}dxdy ,
    \end{equation}
    
    which is the area of the region bounded by the \(x,y\) axes, the vertical and horizontal \(y=60 \text { and } x=60\), respectively, and the lines \(y = x+10 \text { and } y = x-10\), divided by 3600. So we get \( P(\abs{y-x}\le 10 ) = 1100 /3600 \approx 31 \% \).
    
    \item In the second case, for alice we still have \(\rho _{x} = \frac{1}{60 \text{mins} }  \), for Bob we have \(\rho _{y}  = Cy^2 \), where upon normalization \(C = \frac{3}{60^3 \text{mins} } \). The joint probability density function is therefore \( \rho _{x,y} = 3 y^2 / 60^2\). Carrying out the double integral over the same region, we get \( P(\abs{y-x}\le 10 ) = 767 /2592 \approx 30\% \).  
\end{enumerate}~
} 













































\end{document}
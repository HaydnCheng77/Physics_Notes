\documentclass[english,a4paper,12pt]{report}
\usepackage{mypackage}

\title{Calculus}

\author{Haydn Cheng}

\date{\today}

\begin{document}
\maketitle
\tableofcontents

\chapter{Partial Differentiation}

\section{Ordinary Differentiation}

\subsection{The Leibnitz' Theorem} \label{leibnitz} 
\begin{theorem}[The Leibnitz' Theorem]
    The \(n^{\text{th }} \) order derivative of the product of two fucntions \(f(x) = u(x)v(x)\) is\footnote{Proof given in \cref{leibnitzapp}.}  
    \begin{equation}
        f^{(n)} = \sum_{r=0}^{n} \binom{n}{r} u^{(r)} v^{(n-r)}. \label{lei} 
    \end{equation}
\end{theorem}

\subsection{Special Points of Functions}

A stationary piont is characterised by \( df /dx = 0\), and can be further classified into 

\begin{enumerate}
    \item Minimum point: \( d^2f / dx^2 > 0\),
    \item Maximum point: \(d^2f / dx^2 < 0\) and
    \item Inflection point (which is also stationary): \(d^2f / dx^2 = 0 \text { and }  d^2f / dx^2\)  changes sign.
\end{enumerate}

If \(d^2f / dx^2 = 0\) but it does not change sign before and after the stationary point, it could be either of the three cases; we would have to check higher derivatives to verify its nature. 

An inflection point (which is not stationary) is when \(d^2f / dx^2 = 0\) but \(df/dx \neq 0\) where the concavity of the fucntion changes.  

\section{Independent and Dependent Variables}

Variables are some quantities that can change. Relations or constriants are equations that relate the variables and put constriants on how they can change. 

Independent Variables are variables that can change independently without affecting other independent variables. If there are no constriants then all the variables are independent variables and they can change freely without one affecting another.

Dependent Variables are variables that would change when the independent variables change due to constriants. If there are, say, 3 variables \(x,y,z\) and 1 constriant then there would be 1 dependent variables and \(3-1=2\) independent variables. The constriant can be written in the form \(z = z(x,y)\), meaning that \(z\) is the dependent variable that depends on \(x \text { and } y\), which are the independent variables. In other words, \(z\) is a function of \(x \text { and } y\).   

It is important to note that the assignation of independent and dependent variables is entirely arbitrary. As in the case of the above example, we can call any 2 of the 3 variables \(x,y,z\) independent, and the rest change according to the constriant. That is to say that the constriant may also be written in the form \(x(y,z) \text { or } y(x,z)\). 

That said, there is usually an obvious dependent variable, such as the price of an orange \(z\), with obvious independent variables, such as the supply \(x\) and the demand \(y\).  

\section{1 Constriant}

Since the whole point of multivariable calculus is to find how a variable change with another when there are more than the two usual variables \(x \text { and } y\), there is at least one constriant imposed on the variables. 

We start our discussion by considering the bare-minimum -- 3 variables \(x,y,f\) with 1 constriant \(f = f(x,y)\). Here we use the variable \(f\) instead of \(z\) to emphasize that \(f\) is chosen (arbitrary) to be our dependent variable and \(x \text { and } y\) can change independently, \textit{i.e.,} \(f(x,y)\) is a function of \(x \text { and } y\).  

\subsection{Partial Derivatives}

The rate in which \(f\) changes as one of the independent variables, say \(x\), changes while keeping the other, say \(y\), is found by the partial derivative of \(f\) with respect to \(x\) as

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{y} \equiv  \lim_{\Delta x \to 0} \frac{f(x+\Delta x,y) - f(x,y)}{\Delta x}.  
\end{equation}

\subsection{Total Derivatives}

To consider how \(f(x,y)\) changes when both \(x \text { and } y\) change, we can calculate the infinitesimal difference of \(f\) as 

\begin{equation} \label{totaldf} 
    \begin{aligned}
        \Delta f &= f(x+\Delta x,y+\Delta y) - f(x,y) \\
        &= f(x+\Delta x,y+\Delta y) - f(x,y) + f(x,y+\Delta y) - f(x,y+\Delta y) \\
        &= \frac{f(x+\Delta x,y+\Delta y)-f(x,y+\Delta y)}{\Delta x}\Delta x + \frac{f(x,y+\Delta y)-f(x,y)}{\Delta y} \Delta y \\
        &\approx \left(\frac{\partial f}{\partial x} \right)_{y} \Delta x + \left(\frac{\partial f}{\partial y}\right)_{x}  \Delta y.
    \end{aligned}
\end{equation}

As \(\Delta x \to 0 \text { and } \Delta y \to 0\), we have the total differential \(df\) as 

\begin{equation}
	df = \left( \frac{\partial f}{\partial x} \right)_{y} dx + \left( \frac{\partial f}{\partial y} \right)_{x} dy.  
\end{equation}


\section{More than 1 Constriant}

Suppose apart from the constriant (relation to be more precise) that determines \(f = f(x,y)\), we introduce an extra dependent variable \(z = z(x,y)\) as one of the arguement of \(f\), \textit{i.e.,} \(f = f(x,y,z(x,y))\), so there are now 4 variables and 2 constriants in total. 

Again, we can also, say, treat \(y \text { and } z\) as the independent variables and \(x\) as the dependent variable, so it would be misleading to write \(f=f(x,y,z(x,y))\). We would simply bear in mind that \(x,y,z\) related by 1 constriant and write \(f=f(x,y,z)\). 

\subsection{Partial Derivatives}

In the 2 arguments case discussed above, since \(x\) and \(y\) are independent of each other so to find the rate of change in \(f\) with respect to \(x\) only we fix \(y\) to be constant.

Now however, out of the 3 arguments, only two are independent, and the one remaining dependent variable changes as the independent variables change. We therefore have to specifying, when finding the rate of change in \(f\) with respect to \(x\), which other argument, \(y \text { or } z\) are we keeping constant.\footnote{Note that we can instead keep things like \(\sqrt{y+z}\sin (y-z)\) to be constant, and \(y \text { and } z\) change as \(x \text { and } \sqrt{y+z}\sin (y-z)\) change independently. However, this is equivalent to introducing yet another variable and we shall refrain from complicating simple thing. So we wil asuume only \(y \text { or } z\) can be kept constant (when differentiating with respect to \(x\)).}   

If we keep \(y\) to be the independent variable which is kept constant, then the partial derivative of \(f\) with respect to \(x\) is

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{y} \equiv \lim_{\Delta x \to 0} \frac{f(x+\Delta x,y,z(x+\Delta x,y)) - f(x,y,z(x,y))}{\Delta x}. \label{useless} 
\end{equation}

On the other hand if we keep \(z\) to be the independent variable which is kept constant, then the partial deriavtive of \(f\) with respect to \(x\) is 

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{z} \equiv \lim_{\Delta x \to 0} \frac{f(x+\Delta x, y(x+\Delta x,z),z) - f(x,y(x,z),z)}{\Delta x}. 
\end{equation}


In general, if there are \(n\) variables with \(m\) constraints, then we have to specify which \(n-m-1\) variables are kept constant while finding the derivative with respect to one of the \(n\) variables. 

For instance if we have the function and constriant as 

\begin{equation}
	f = f(x,y,z,t), \quad xy = zt,
\end{equation}

then only 3 out of the 5 variables \(x,y,z,t,f\) can be independent; the fourth and fifth are then determined through the equation \(xy = zt \text { and } f = f(x,y,z,t)\). Thus we would write expressions like

\begin{equation}
	\left( \frac{\partial f}{\partial x}  \right)_{y,t} = \text{partial derivative of \(f\) with respect to \(x\) while keeping \(y\) and \(t\) constant.}  
\end{equation}

\subsection{Formal Partial Derivatives}

The formal partial derivative is to carry out a partial derivative assuming that all the arguments are indepndent of each other and keeping all of them constant except for the variable under differentiation. For example, the formal partial derivative of \(f(x,y,z)\) with respect to \(x\) is 

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{y,z} \equiv f_{x} \equiv  \lim_{\Delta x \to 0} \frac{f(x+\Delta x,y,z) - f(x,y,z)}{\Delta x}.
\end{equation}

This form of partial derivative becomes handy when discussing the total derivative when the arguments are related.

\subsection{Total Derivatives}

To consider how \(f(x,y,z(x,y))\) changes when \(x,y \text { and } z\) change, we can calculate the infinitesimal difference of \(f\) as 

\begin{equation}
	\begin{aligned} 
	\Delta f &= f(x+\Delta x,y+\Delta y,z+\Delta z) - f(x,y,z) \\
	&= f(x+\Delta x,y+\Delta y,z+\Delta z) - f(x,y,z) \\
	&+ f(x,y+\Delta y,z+\Delta z) -f(x,y+\Delta y,z+\Delta z) \\
	&+ f(x,y,z+\Delta z) - f(x,y,z+\Delta z)\\
	&= \frac{f(x+\Delta x,y+\Delta y,z+\Delta z) - f(x,y+\Delta y,z+\Delta z)}{\Delta x} \Delta x \\
	&+ \frac{f(x,y+\Delta y,z+\Delta z)-f(x,y,z+\Delta z)}{\Delta y}\Delta y \\
	&+ \frac{f(x,y,z+\Delta z)-f(x,y,z)}{dz} \Delta z\\
	& \approx  \left( \frac{\partial f}{\partial x} \right)_{y,z} \Delta x + \left( \frac{\partial f}{\partial y} \right)_{x,z} \Delta y + \left( \frac{\partial f}{\partial z} \right)_{x,y} \Delta z.  
	\end{aligned} 
\end{equation}

As \(\Delta x \to 0, \Delta y \to 0 \text { and } \Delta z \to 0\), we have the total differential \(df\) as 

\begin{equation} \label{totaldf} 
	df = \left( \frac{\partial f}{\partial x} \right)_{y,z} dx + \left( \frac{\partial f}{\partial y} \right)_{x,z} dy + \left( \frac{\partial f}{\partial z} \right)_{x,y} dz = f_{x} dx + f_{y}dy + f_{z}dz.     
\end{equation}

It is important to note that the partial derivatives involved in the total differential \(df\) are formal partial derivatives but not actual partial derivatives, although \(x,y,z\) might depend on each other.  

\subsection{Chain Rule}

We are too close to introducing the chain rule in multivariable calculus that it make no sense to jump to the next topic.

If we want to evaluate the partial deriavtive of \(f(x,y,z)\) with repsect to \(x\) while keeping \(y\) constant (and \(z\) be whatever \(z = z(x,y)\) be), one can in principle use \cref{useless}. However, a faster and more pratical method is to use the chain rule, which is when we divide both sides in \cref{totaldf} by \(dx\) while keeping \(y\) constant

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{y} = \left( \frac{\partial f}{\partial x} \right)_{y,z} + \left( \frac{\partial f}{\partial z} \right)_{x,y} \left( \frac{\partial z}{\partial x} \right)_{y} = f_{x} + f_{z} \left( \frac{\partial z}{\partial x} \right)_{y}. 
\end{equation}

If we use the form \(f(x,y,z) = f(x(z),y(z),z) = f(z)\), then \(f_{x} = 0 \) and the above equation becomes reminiscent to the ordinary chain rule

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{y} = \left( \frac{\partial f}{\partial z} \right)_{x,y} \left( \frac{\partial z}{\partial x} \right)_{y}. 
\end{equation}

The same formula can be more easily obtain if we first write the differential of \(f\), \textit{i.e.,} \(df\) in terms of \(z\), \textit{i.e.,} \(df = f_{z}dz \).  

If for example, instead, we have 3 constriants for the 4 variables, namely \(f = (x,y,z), x=x(z) \text { and } y=y(z)\), then we can find the partial (now ordinary) derivative of \(f\) with respect to \(z\) as

\begin{equation}
	\frac{df}{dz} = \left( \frac{\partial f}{\partial x} \right)_{y,z} \frac{dx}{dz} + \left( \frac{\partial f}{\partial y} \right)_{x,z} \frac{dy}{dz} + \left( \frac{\partial f}{\partial z} \right)_{x,y} = f_{x} \frac{dx}{dz} + f_{y} \frac{dy}{dz} + f_{z}.
\end{equation}



\section{Exact Differentials}

Before discussing how to evaluate partial derivatives when the arguments are dependent, we mention the concept of exact differentials, which is closely related to the concept of total derivatives.

An arbitrary differential 

\begin{equation}
    A(x,y) dx + B(x,y) dy
\end{equation}

is exact if it is the total derivative of a function \(f(x,y)\) 

\begin{equation}
    df(x,y) = \left(\frac{\partial f}{\partial x}\right)_{y}  dx + \left(\frac{\partial f}{\partial y}\right)_{x} dy.
\end{equation}

Therefore, we have

\begin{equation}
    A(x,y) = \frac{\partial f}{\partial x} ~\text { and }~ B(x,y) = \frac{\partial f}{\partial y}
\end{equation}

Since \( \partial ^2f / \partial x \partial y = \partial ^2f / \partial y \partial x\), we obtain a necessary (and a sufficient) condition for the differential to be exact, which is

\begin{equation}
    \frac{\partial A}{\partial y} = \frac{\partial B}{\partial x}.\footnote{Determining whether a differential containing many variables \(x_1, x_2, \ldots, x_{n}\) is exact is a simple extension of the above: a differential \(df = \sum_{i=1}^{n} g_{i}(x_1 ,x_2 ,\ldots , x_{n} )dx_{i}  \) is exact if \(\partial g_{i} /\partial x_{j}  = \partial g_{j} / \partial x_{i}   \) for all pairs \(i,j\).}
\end{equation}


\section{Elimination, Chain Rule and Differentials} \label{chain} 

There are three ways to carry out partial derivatives when the arguments are dependent, namely elimination, chain rule and differentials.

This is best illustrated by an example. Suppose we have the function

\begin{equation}
	f(x,y) = x^2+y^2+(x^2+y^2)^2
\end{equation}

Now we introduce an extra variable \(z(x,y) = x^2+y^2\), so that \(f\) now can be rewritten in virtually infinite ways

\begin{equation}
	f(x,y,z) = x^2+y^2+z^2 = z+z^2 = x^2+y^2+z(x^2+y^2) \quad \textit{etc.} 
\end{equation}

We, in the next few subsections are going to find the partial derivatives of \(f\) with respect to \(x \text { and }  z\)\footnote{\(x \text { and } y\) are symmetric so the partial derivatives of \(f\) respect to \(y\) is neglected.}  using the three methods mentioned. 

\subsection{Elimination}

To evaluate the partial derivative of \(f\) with respect to \(x\) while keeping \(y\) constant (and let \(z\) be whatever \(z = x^2+y^2\) is), we eliminate \(z\) from \(z = z(x,y)\) and differentiate the function \(f=f(x,y)\)  

\begin{equation}
	\left( \frac{\partial f}{\partial x}  \right)_{y} = \frac{\partial }{\partial x} (x^2+y^2+(x^2+y^2)^2) = 2x+4x^3 +4xy^2.  
\end{equation}

To evaluate the partial derivative of \(f\) with respect to \(x\) while keeping \(z\) constant (and let \(y\) be whatever \(y = \pm \sqrt{z-x^2} \) is), we eliminate \(y\) from \(y = y(x,z)\) and differentiate the function \(f=f(x,z)\) 

\begin{equation}
	\left( \frac{\partial y}{\partial x} \right)_{z} = \frac{\partial }{\partial x}(x^2+(z-x^2)+z^2) = 0.
\end{equation}


To evaluate the partial derivative of \(f\) with respect to \(z\) while keeping \(x\) (or \(y\)) constant (and let \(y\) (or \(x\)) be whatever \(y = \pm \sqrt{z-x^2} \) is), we eliminate \(y\) from \(y = y(x,z)\) and differentiate the function \(f = f(x,z)\)

\begin{equation}
	\left( \frac{\partial f}{\partial z} \right)_{x} = \frac{\partial }{\partial z} (x^2+(z-x^2)+z^2) = 1+2z. 
\end{equation}

\subsection{Chain Rule}

To use the chain rule we first write down the total differential of \(z(x,y)\), \textit{i.e.,} \(dz\) as

\begin{equation}
	dz = z_{x} dx + z_{y} dy = 2x dx + 2ydy.   
\end{equation}

Now we divide the whole equation by \(dx\) while keeping \(y\) constant to get 

\begin{equation}
	\left( \frac{\partial z}{\partial x} \right)_{y} = 2x.
\end{equation}

If we instead divide the whole equation by \(dx\) while keeping \(z\) constant we get

\begin{equation}
	\left( \frac{\partial y}{\partial x} \right)_{z} = -\frac{x}{y}.  
\end{equation}

Dividing the whole equation by \(dz\) while keeping \(x\) constant yields

\begin{equation}
	\left( \frac{\partial y}{\partial z} \right)_{x} = \frac{1}{2y}.
\end{equation}




We can now find the partial derivative of \(f\) with respect to \(x\) while keeping \(y\) constant

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{y} = \frac{\partial }{\partial x}  (x^2+y^2+z^2) = 2x + 2z \left( \frac{\partial z}{\partial x} \right)_{y} = 2x + 2(x^2+y^2) (2x) = 2x + 4x^3 + 4xy^2.
\end{equation}


The partial derivative of \(f\) with respect to \(x\) while keeping \(z\) constant is

\begin{equation}
	\left( \frac{\partial f}{\partial x} \right)_{z} = \frac{\partial }{\partial x} (x^2+y^2+z^2) = 2x + 2y \left( \frac{\partial y}{\partial x} \right)_{z} = 2x + 2\left( -\frac{x}{y}  \right)  = 0.
\end{equation}

The partial derivative of \(f\) with respect to \(z\) while keeping \(x\) constant is


\begin{equation}
	\left( \frac{\partial f}{\partial z} \right)_{x} = \frac{\partial }{\partial z} (x^2+y^2+z^2) = 2y \left( \frac{\partial y}{\partial z} \right)_{x} + 2z = 2y \left( \frac{1}{2y} \right) + 2z = 2+2z.
\end{equation}

\subsection{Differentials}

To use the differentials method we write down the differential forms of the constriants

\begin{equation}
	df = 2x dx + 2y dy + 2z dz,
\end{equation}

and 

\begin{equation}
	dz = 2x dx + 2y dy.
\end{equation}

To find the partial derivative of \(f\) with respect to \(x\) while keeping \(y\) constant we eliminate the differential \(dz\) to get 

\begin{equation}
	df = (2x + 4x(x^2+y^2))dx + (2y+4y(x^2+y^2))dy \implies \left( \frac{\partial f}{\partial x} \right)_{y} = 2x + 4x^3 + 4xy^2.
\end{equation}

To find the partial derivative of \(f\) with respect to \(x\) while keeping \(z\) constant we eliminate the differential \(dy\) to get 

\begin{equation}
	df = (2z+1)dz \implies \left( \frac{\partial f}{\partial x} \right)_{z} = 0. 
\end{equation}

TO find the partial derivative of \(f\) with respect to \(z\) while keeping \(x\) constant we eliminate the differential \(dy\) to get 

\begin{equation}
	df = (2z+1)dz \implies \left( \frac{\partial f}{\partial z} \right)_{x} = 1+2z.
\end{equation}


\subsection{Intepretation}

To visualize the results, we refer to \cref{paraboloid}, which shows the relation \(z = x^2+y^2\), and the function \(f(x,y,z)\) calculates the square of the distance from the origin. 

Suppose we want to calculate the partial derivative of \(f(x,y,z)\) with respect to \(x\) at the point \(P_0 = (1,0,1)\). If we hold \(y\) fixed and let \(x\) varies, then the point \(P\) moves in the \(xz\)-plane towards \(A\), and the distance increases as \(x\) increases. However, if we hold \(z\) fixed and let \(x\) varies, then point \(P\) moves in the plane \(z = 1\), along the circular path towards \(B\) and the distance is fixed.   

\onefig{paraboloid}{scale=0.3} 



\example{Partial and Total Derivatives (1).}
{Find \( \left( \partial w / \partial y  \right)_{x,t} \), where 

\begin{equation}
	w(x,y,z,t) = x^3 y-z^2t \text { and } xy = zt.
\end{equation}
}
{With chain rule, we have

\begin{equation}
	\left( \frac{\partial w}{\partial y}  \right)_{x,t} = x^3 -2zt \left( \frac{\partial z}{\partial y}  \right)_{x,t} = x^3 - 2zx.  
\end{equation}

Alternatively, we have

\begin{equation}
	dw = 2x^2ydx+x^3 dy+2zt dz-z^2dt ~\text { and }~ ydx+xdy=zdt+tdz.
\end{equation}

Eliminating \(dz\), we have 

\begin{equation}
	dw = (3x^2y-2zy)dx + (x^3 - 2zx)dy+z^2dt. 
\end{equation}

Comparing with the total derivative of \(w(x,y,t)\) 

\begin{equation}
	dw = \left( \frac{\partial w}{\partial x}  \right)_{y,t} dx + \left( \frac{\partial w}{\partial y}  \right)_{x,t} dy +  \left( \frac{\partial w}{\partial t}  \right)_{x,y} dt ,  
\end{equation}

we obtain the same answer.
} 

\example{Partial and Total Derivatives (2).}
{Find \((\partial w /\partial x)_{y} \text { and } (\partial w /\partial y)_{x} \), where

\begin{equation}
	w = x^2-yz+t^2, \quad z^2=x+y^2 ~\text { and }~ xy = zt.
\end{equation}
}
{Taking the differential of the three above equations we have

\begin{equation}
	dw = 2xdx - zdy - ydz + 2tdt, \quad x dy + ydx = zdt + tdz ~\text { and }~ 2zdz = dx + 2ydy.
\end{equation}

Elminating \(dt \text { and } dz\), we have

\begin{equation}
	dw = \left( 2x - \frac{y}{2z} + \frac{2ty}{z} - \frac{t^2}{z^2}    \right) dx + \left( -z - \frac{y^2}{z} + \frac{2xt}{z} - \frac{2t^2y}{z^2}    \right) dy.
\end{equation}

Comparing with the total derivative of \(w(x,y)\) 

\begin{equation}
	dw = \left( \frac{\partial w}{\partial x}  \right)_{y} dx + \left( \frac{\partial w}{\partial y}  \right)_{x} dy, 
\end{equation}

we obtain the desired solutions.

It would be very messy if we try to compute the partial derivatives directly like the first method in the above example.

} 

\example{Partial and Total Derivatives (3).}
{Suppose the variables \(x,y,z\) satisfy an equation \(g(x,y,z) = 0\) and the point \(P = (1,1,1)\) lies on the surface \(g=0\) with \((\grad{g} )_{P} = (-1,1,2) \).

Let \(f(x,y,z)\) be another function and assume that \((\grad{f} )_{P} = (1,2,1) \). 

Find the gradient of the function \(w = f(x,y,z(x,y))\) of the two independent variables \(x \text { and } y\) at the point \(x = 1 \text { and } y = 1\).     }
{In differential forms we have

\begin{equation}
	(d w)_{P} = dx+2dy+dz, \quad (dg)_{P} = -dx+dy+2dz = 0.  
\end{equation}

Eliminating \(dz\) we have 

\begin{equation}
	(dw)_{P}= \frac{3}{2}dx + \frac{3}{2}dy \implies (\grad{w} )_{P}= \frac{3}{2}\vu{i} + \frac{3}{2}\vu{j} .      
\end{equation}
} 

\example{Formal and Actual Partial Derivatives (1).}
{Suppose \(w = w(x,r)\), with \(r = r(x,\theta )\). Give an expression for \(\left( \partial w /\partial r \right)_{\theta } \) in terms of formal partial derivatives of \(w \text { and } r\).  }
{The total derivative of \(w\) with \(r \text { and } \theta \) being the independent variables is 

\begin{equation}
	dw = w_{x} dx + w_{r} dr. 
\end{equation}

Dividing the equation by \(dr\) while keeping \(\theta \) constant, we have 

\begin{equation}
	\left(\frac{\partial w}{\partial r}\right) _{\theta } = w_{x} \left( \frac{\partial x}{\partial r}  \right)_{\theta } + w_{r} =  w_{x}\left( \frac{\partial r}{\partial x}  \right)^{-1} _{\theta } + w_{r} = \frac{w_{x} }{r_{x} } + w_{r}.    
\end{equation}
~
} 

\example{Formal and Actual Partial Derivatives (2).}
{If \(f(x,y,z) = xy^2z^{4} \), where \(z = 2x+3y\), find the three formal derivatives \(f_{x},f_{y} \text { and } f_{z}   \)  and the three of the many possible actual partial derivatives \((\partial f/\partial x)_{y}, (\partial f /\partial y)_{x} \text { and } (\partial f / \partial z)_{x}  \).}
{The three formal derivatives are 

\begin{equation}
	f_{x}= y^2z^{4}, \quad f_{y} = 2xyz^{4} ~\text { and }~ f_{z} = 4xy^2z^3 .     
\end{equation}

The three actual partial derivatives are 

\begin{equation}
	\begin{cases}
		\begin{aligned}
			\left( \frac{\partial f}{\partial x}  \right)_{y} &= f_{x} + f_{z}\left( \frac{\partial z}{\partial x}  \right)_{y} = y^2z^{4}+8xy^2z^3,\\
			\left( \frac{\partial f}{\partial y}  \right)_{x} &= f_{y} + f_{z}\left( \frac{\partial z}{\partial y}  \right)_{x} = 2xyz^{4}+12xy^2z^3,\\
			\left( \frac{\partial f}{\partial z}  \right)_{x} &= f_{y}\left( \frac{\partial y}{\partial z}  \right)_{x} + f_{z} = \frac{2}{3}xyz^{4}+4xy^2z^3, 
		\end{aligned}s
	\end{cases}
\end{equation}

where we have used the chain rule at each line.
} 




\section{Symmetry of Mixed Derivatives, Reciprocity Relation and Cyclic Relation}

A very useful fact about the second partial derivative which the proof is not given here (but is intuitive to understand) is the symmetry of mixed derivatives

\begin{equation}
    \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial }{\partial x} \left( \frac{\partial f}{\partial y}  \right) = f_{xy} = f_{yx} = \frac{\partial }{\partial y} \left( \frac{\partial f}{\partial x}  \right) = \frac{\partial^2 f}{\partial y \partial x}.     
\end{equation}

So far our discussion has centred on a function \(f(x,y)\) dependent on two variables, \(x\text { and } y\). However, \(f(x,y)\) is not superior, \(x(f,y) \text { and } y(x,f)\) are equally valid and are expressing the identical relation between \(x,y \text { and } f\). To emphasise the point that all the variables are of equal standing we now replace \(f\) by \(z\). Then we have

\begin{equation}
    \begin{aligned}
    dx &= \left( \frac{\partial x}{\partial y}  \right)_{z} dy + \left( \frac{\partial x}{\partial z}  \right)_{y} dz ~\text { and }~   dy = \left( \frac{\partial y}{\partial x}  \right)_{z} dx + \left( \frac{\partial y}{\partial z}  \right)_{x} dz \\
    \implies dx &= \left( \frac{\partial x}{\partial y}  \right)_{z} \left( \frac{\partial y}{\partial x}  \right)_{z} dx + \left( \left( \frac{\partial x}{\partial y}  \right)_{z}  \left( \frac{\partial y}{\partial z}  \right)_{x} + \left( \frac{\partial x}{\partial z}  \right)_{y}  \right) dz. 
    \end{aligned}
\end{equation}

Since \(dx \text { and }  dz\) are independent, we the reciprocity and the cyclic relations

\begin{equation}
    \left( \frac{\partial x}{\partial y}  \right)_{z} = \left( \frac{\partial y}{\partial x}  \right)_{z}^{-1} ~\text { and }~ \left( \frac{\partial y}{\partial z}  \right)_{x} \left( \frac{\partial z}{\partial x}  \right)_{y} + \left( \frac{\partial x}{\partial y}  \right)_{z} = -1 
\end{equation}

The reciprocity and the cyclic relations can be generalized to higher number of variables. For the reciprocity relation, we simply keep linking each numerator with the previous denominator while keeping the same constraint, \textit{e.g.,} 

\begin{equation}
	\left( \frac{\partial f}{\partial x}  \right)_{y} \left( \frac{\partial x}{\partial z}  \right)_{y} \left( \frac{\partial z}{\partial f}  \right)_{y} = 1 \implies \left( \frac{\partial f}{\partial x}  \right)_{y}  = \left( \frac{\partial f}{\partial z}  \right)_{y} \left( \frac{\partial z}{\partial x}  \right)_{y}.  
\end{equation}

Some call this the chain rule in multivariable calculus.

For the cyclic relation, the further variables simply appear as ``spectators'' which are held constant in all the terms, \textit{e.g.,} 

\begin{equation}
	\left( \frac{\partial f}{\partial y}  \right)_{x,z} \left( \frac{\partial y}{\partial x}  \right)_{f,z}\left( \frac{\partial x}{\partial f}  \right)_{y,z} = -1.  
\end{equation}



\section{Change of Variables}

\subsection{Change of Gradients}

In \cref{chain}, we have discussed the case for 3 arguments with 2 constriants. We, however, would mostly deal with the case for 4 arguments with 2 constriants, when we try to change the whole set of independent variables from \((x,y)\) to \((u,v)\). The two constriants in this case can be expressed as \(x=x(u,v) \text { and } y=y(u,v)\). To find the partial derivatives of \(f\) with respect to \(u \text { and } v\) are, we first write down the total derivative of \(f\), \textit{i.e.,} \(df\), with respect to all the arguments \(x,y,u\text { and } v\)

\begin{equation}
	df = f_{x}  dx + f_{y}  dy + f_{u} du + f_{v} dv.    
\end{equation}

Now we divide the whole equation by \(du\) while keeping \(v\) constant 

\begin{equation} \label{changeofvar} 
    \left( \frac{\partial f}{\partial u}  \right)_{v} = f_{x}  \left( \frac{\partial x}{\partial u}  \right)_{v}  + f_{y}  \left( \frac{\partial y}{\partial u}  \right)_{v},
\end{equation}

where the term \((\partial f/\partial u)_{v} \) vanishes since \(f(x,y,u,v) = x^2+y^2\) has no explicit \(u\)-dependence. One might argue that \(f(x,y,u,v) = u+v\) has \(u\)-dependence. However, we have to use the same form of \(f\) throughout. Since we have already assumed \(f_{x} \neq 0 \text { and } f_{y} \neq 0  \), we have implicitly assumed that we are using \(f(x,y,u,v) = x^2+y^2\) instead of \(f(x,y,u,v) = u+v\). If instead we use the latter expression then we would get something like \((\partial f /\partial u )_{v} = f_{u}  \), which is trivial.

Similarly we divide the whole equation by \(dv\) while keeping \(u\) constant

\begin{equation}
	\left( \frac{\partial f}{\partial v}  \right)_{u} = f_{x} \left( \frac{\partial x}{\partial v}  \right)_{u} + f_{y}  \left( \frac{\partial y}{\partial v}  \right)_{u},
\end{equation}

where \((\partial f/\partial v)_{u} \) vanishes since \(f(x,y,u,v) = x^2+y^2\) has no explicity \(v\)-dependence.  

To generalize the above discussion, suppose we want to change the set of \(n\) variables from \(\vb{x} = (x_1 ,x_2 ,\ldots ,x_{n} )\) to \(\vb{u} =(u_1 ,u_2 ,\ldots ,u_{n} ) \), then the \(n\) equations are 

\begin{equation}
	\frac{\partial f}{\partial u_{i} } = f_{x_1 } \frac{\partial x_1}{\partial u_{i} } + f_{x_2 } \frac{\partial x_2 }{\partial u_{i} } + \cdots + f_{x_{n} } \frac{\partial x_{n} }{\partial u_{i} } = \sum_{j=1}^{n} f_{x_{j} } \frac{\partial x_{j} }{\partial u_{i} }.
\end{equation}

In matrix form this is

\begin{equation}
	\grad{}_{\vb{u} } = \left( \frac{\partial }{\partial u_1}, \frac{\partial }{\partial u_2 }, \ldots ,\frac{\partial }{\partial u_{n} }  \right) = J^T \grad{}_{\vb{x} } = J^T \left( \frac{\partial }{\partial x_1 }, \frac{\partial }{\partial x_2 }, \ldots , \frac{\partial }{\partial x_{n} }    \right),
\end{equation}

where \(J\) is the Jacobian matrix 

\begin{equation}
	J \equiv \frac{\partial (x,y,z)}{\partial (u,v,w)} = \begin{pmatrix}
		\partial x/\partial u  & \partial x/\partial v  & \partial x/\partial w   \\
		\partial y/\partial u  & \partial y/\partial v  & \partial y/\partial w   \\
		\partial z/\partial u  & \partial z/\partial v  & \partial z/\partial w   \\
		\end{pmatrix} = \begin{pmatrix}
			x_{u}  & x_{v}  & x_{w}   \\
			y_{u}  & y_{v}  & y_{w}   \\
			z_{u}  & z_{v}  & z_{w}   \\
		\end{pmatrix},
\end{equation}

where we have assumed, for example, that \(v \text { and } w\) are kept constant in \(\partial x /\partial u \).  

In three dimensions, the 3 equations are

\begin{equation} \label{jacobderi}
		\begin{pmatrix}
			\partial/\partial u \\
			\partial/\partial v \\
			\partial/\partial w
		\end{pmatrix} 
		=
		\begin{pmatrix}
			x_{u} & y_{u} & z_{u} \\
			x_{v} & y_{v} & z_{v} \\
			x_{w} & y_{w} & z_{w}     
		\end{pmatrix}
		\begin{pmatrix}
			\partial/\partial x \\
			\partial/\partial y \\
			\partial/\partial z
		\end{pmatrix} .	
\end{equation} 

\subsection{Change of Areas and Volumes}

From \(x=x(u,v) \text { and } y=y(u,v)\) we have 

\begin{equation}
	\begin{cases}
		dx = x_{u} du + x_{v}  dv \\
		dy = y_{u} du + y_{v}  dv.  
	\end{cases}
\end{equation}

In matrix form,

\begin{equation}
	d\vb{r} =
	\begin{pmatrix}
		 dx \\
		 dy \\
	\end{pmatrix} = \begin{pmatrix}
		x_{u}   & x_{v}    \\
		y_{u}   & y_{v}    \\
	\end{pmatrix} \begin{pmatrix}
		 du \\
		 dv \\
	\end{pmatrix}.
\end{equation}

On the \(x\)-\(y\) plane, the infinitesimal displacement vector which keeps \(v \text { and } u\) constant are, respectively

\begin{equation} \label{dr} 
	d\vb{r} _{u} = \begin{pmatrix}
		 x_{u}  \\
		 y_{u}   \\
	\end{pmatrix}du  ~\text { and }~ d\vb{r} _{v} = \begin{pmatrix}
		 x_{v}  \\
		 y_{v}  \\
	\end{pmatrix}dv.
\end{equation}

To relate the area \(dA_{x,y} = dxdy = dA_{u,v}\),\footnote{Note that the \(dA_{u,v} = dA_{x,y} \) since both refers to an infinitesimal area in the \(x\)-\(y\) plane. Formally, we have \(\det (d\vb{r} _{u},d\vb{r} _{v}  ) = \det (\mathbb{I})dxdy = dxdy\) if we use the former expression for \(d\vb{r} \) in \cref{dr} instead of the latter.} we have 

\begin{equation}
	dA_{u,v} = \det  (d\vb{r} _{u},d\vb{r} _{v}  ) = \begin{vmatrix}
		x_{u}  & x_{v}   \\
		y_{u}  & y_{v}   \\
	\end{vmatrix} dudv = \left|\frac{\partial (x,y)}{\partial (u,v)}\right|dudv. 
\end{equation}

For volume we have

\begin{equation}
	dV_{u,v,w} = \left| \frac{\partial (u,v,w)}{\partial (x,y,z)}\right| dudvdw.  
\end{equation}

When changing variables in multiple integral, the limits of integration has to change accordingly by finding the maximum and minimum values for \(u \text { and } v\) in the same region. 

\subsection{Multiple Jacobians}

For three sets of variables \(\vb{x} ,\vb{y} \text { and } \vb{z} \), we have, from chain rule

\begin{equation}
	\frac{\partial x_{i} }{\partial z_{j} } = \sum_{k=1}^{n} \frac{\partial x_{i} }{\partial y_{k} } \frac{\partial y_{k} }{\partial z_{j} }.   
\end{equation}

Now let \(A, B \text { and } C\) as the matrices whose \(ij^{\text{th }} \) elements are \(\partial x_{i} / \partial y_{j} , \partial y_{i} / \partial z_{j}  \text { and } \partial x_{i}/ \partial z_{j} \) respectively. We can then rewrite the above equation as 

\begin{equation}
	c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj} \implies C = AB \implies \det (C) = \det (A) \det (B) \implies J_{xz} = J_{xy} J_{yz}.   
\end{equation}

In the special case where \(z_{i} = x_{i}  \), we get \(J_{xy}J_{yx} =1  \).  

\example{Jacobian Rule (1).}
{Prove the first Jacobian rule 

\begin{equation}
	\left( \frac{\partial u}{\partial x}  \right)_{v} \left( \frac{\partial v}{\partial y}  \right)_{x} = \left| \frac{\partial (u,v)}{\partial (x,y)} \right|.
\end{equation}
~
}
{We start with the total derivative of \(u\) in terms of \(x \text { and } y\)

\begin{equation}
	du = \left( \frac{\partial u}{\partial x}  \right)_{y} dx + \left( \frac{\partial u}{\partial y} \right)_{x} dy.  
\end{equation}

Dividing the whole equation by \(dx\) while keeping \(v\) constant, we have

\begin{equation}
	\left( \frac{\partial u}{\partial x}  \right)_{v} = \left( \frac{\partial u}{\partial x}  \right)_{y} + \left( \frac{\partial u}{\partial y}  \right)_{x} \left( \frac{\partial y}{\partial x}  \right)_{v}. 
\end{equation}

We then write down the total derivative of \(v\) in terms of \(x \text { and } y\)

\begin{equation}
	dv = \left( \frac{\partial v}{\partial x}  \right)_{y} dx + \left( \frac{\partial v}{\partial y}  \right)_{x} dy.  
\end{equation}

Dividing the whole equation by \(dx\) while keeping \(v\) constant, we have

\begin{equation}
	\left( \frac{\partial v}{\partial x}  \right)_{v}  =0 = \left( \frac{\partial v}{\partial x}  \right)_{y} + \left( \frac{\partial v}{\partial y}  \right)_{x} \left( \frac{\partial y}{\partial x}  \right)_{v}.    
\end{equation}

Solving for \(\left( \partial y /\partial x \right)_{v} \) gives

\begin{equation}
	\left( \frac{\partial y}{\partial x}  \right)_{v} = -\left( \frac{\partial v}{\partial x}  \right)_{y} \left( \frac{\partial v}{\partial y}  \right)^{-1} _{x}.  
\end{equation}

Substituting back into \(\left( \partial u/\partial x \right)_{v} \), we have

\begin{equation}
	\left( \frac{\partial u}{\partial x}  \right)_{v} = \left( \frac{\partial u}{\partial x}  \right)_{y} - \left( \frac{\partial u}{\partial y}  \right)_{x} \left( \frac{\partial v}{\partial x}  \right)_{y} \left( \frac{\partial v}{\partial y}  \right)^{-1} _{x}. \label{imm} 
\end{equation}

Rearranging we get

\begin{equation}
	\left( \frac{\partial u}{\partial x}  \right)_{v} \left( \frac{\partial v}{\partial y}  \right)_{x} = \left( \frac{\partial u}{\partial x}  \right)_{y} \left( \frac{\partial v}{\partial y}  \right)_{x} - \left( \frac{\partial u}{\partial y}  \right)_{x} \left( \frac{\partial v}{\partial x}  \right)_{y},      
\end{equation}

as desired.
} 

\example{Jacobian Rule (2).}
{Prove the second Jacobian rule

\begin{equation}
	\left( \frac{\partial u}{\partial x}  \right)_{v} = \left( \frac{\partial u}{\partial x}  \right)_{y} - \left( \frac{\partial u}{\partial v}  \right)_{x} \left( \frac{\partial v}{\partial x}  \right)_{y}.    
\end{equation}
~
}
{We start an intermediate step (\cref{imm}) from the above exmample

\begin{equation}
	\left( \frac{\partial u}{\partial x}  \right)_{v} = \left( \frac{\partial u}{\partial x}  \right)_{y} - \left( \frac{\partial u}{\partial y}  \right)_{x} \left( \frac{\partial v}{\partial x}  \right)_{y} \left( \frac{\partial v}{\partial y}  \right)^{-1} _{x}.
\end{equation}

To relate \(\left( \partial u /\partial y \right)_{x} \) to \(\left( \partial u/\partial v \right)_{x} \), we have the chain rule

\begin{equation}
	\left( \frac{\partial u}{\partial v} \right)_{x} = \left( \frac{\partial u}{\partial y} \right)_{x} \left( \frac{\partial y}{\partial v} \right)_{x} \implies \left( \frac{\partial u}{\partial y} \right)_{x} = \left( \frac{\partial u}{\partial v} \right)_{x} \left( \frac{\partial v}{\partial y} \right)_{x}.
\end{equation}

Substitute this into the intermediate step, we have

\begin{equation}
	\left( \frac{\partial u}{\partial x} \right)_{v} = \left( \frac{\partial u}{\partial x} \right)_{y} - \left( \left( \frac{\partial u}{\partial v} \right)_{x} \left( \frac{\partial v}{\partial y} \right)_{x}  \right) \left( \frac{\partial v}{\partial x} \right)_{y} \left( \frac{\partial v}{\partial y} \right)_{x} ^{-1} = \left( \frac{\partial u}{\partial x} \right)_{y} - \left( \frac{\partial u}{\partial v} \right)_{x} \left( \frac{\partial v}{\partial x} \right)_{y},
\end{equation}

as desired.
}

\example{Jacobian Rule (3).}
{Prove the third Jacobian rule

\begin{equation}
	\left( \frac{\partial u}{\partial v} \right)_{w} = \left| \frac{\partial (u,w)}{\partial (x,y)}\right| \left|\frac{\partial (v,w)}{\partial (x,y)}\right|^{-1} .  
\end{equation}
~
}
{When \(w\) is held constant, we have

\begin{equation}
	dw = w_{x}dx+w_{y}dy =0 \implies \frac{dy}{dx} = -\frac{w_{x} }{w_{y} }.    
\end{equation}

Therefore we have 

\begin{equation}
	\left( \frac{\partial u}{\partial x} \right)_{w} = u_{x} + u_{y} \frac{dy}{dx} = u_{x} - u_{y} \frac{w_{x} }{w_{y} },      
\end{equation}

and similarly 

\begin{equation}
	\left( \frac{\partial v}{\partial x} \right)_{w} = v_{x} + v_{y}\frac{dy}{dx} = v_{x} - v_{y} \frac{w_{x} }{w_{y} }.      
\end{equation}

From chain rule we therefore get

\begin{equation}
	\left( \frac{\partial u}{\partial v} \right)_{w} = \left( \frac{\partial u}{\partial x} \right)_{w} \left( \frac{\partial v}{\partial x} \right)_{w} ^{-1} = \frac{u_{x}w_{y}-u_{y}w_{x}    }{v_{x}w_{y}-v_{y}w_{x}    } =\left| \frac{\partial (u,w)}{\partial (x,y)}\right| \left|\frac{\partial (v,w)}{\partial (x,y)}\right|^{-1},  
\end{equation}

as desired.
} 


\example{Thermodynamics.}
{In thermodynamics, some of the variables associated with a confined gas are \(p\)(pressure), \(V\)(volume), \(T\)(temperature), \(U\)(internal energy), \(S\)(entropy) and \(H\)(enthalpy). Any two are independent and their values then determine all the others. 

Suppose there is a thermogynamics law 

\begin{equation}
	\left( \frac{\partial U}{\partial p}  \right)_{T} + T \left( \frac{\partial V}{\partial T}  \right)_{p} + p \left( \frac{\partial V}{\partial p}  \right)_{T} = 0, 
\end{equation}

where \(p \text { and } T\) are the independent variables in this equation since they appear at the denominators. 

What is the equation for this law when \(V \text { and } T\) are the independent variables? 
}
{Using chain rule, cyclic relation and reciprocity relation on the three terms respectively we have 

\begin{equation}
	\left( \frac{\partial U}{\partial V}  \right)_{T} \left( \frac{\partial V}{\partial p}  \right)_{T} - T \left( \frac{\partial p}{\partial T}  \right)_{V}\left( \frac{\partial V}{\partial p}  \right)_{T} + p \left( \frac{\partial V}{\partial p}  \right)_{T} = 0 = \left( \frac{\partial U}{\partial V}  \right)_{T} - T\left( \frac{\partial p}{\partial T}  \right)_{V} + p .  
\end{equation}

} 


\section{Taylor's Theorem}
When \(\Delta x \text { and }  \Delta y\) are finite, we can no longer neglect the terms which are not linear in \(\Delta x \text { or } \Delta y\) in \cref{totaldf}, instead, we get the taylor series 

\begin{equation}
    \begin{aligned}
    f(x,y) &= f(x_0 ,y_0 ) + \eval{\frac{\partial f}{\partial x}}_{(x_0 ,y_0 )}  \Delta x + \eval{\frac{\partial f}{\partial y}}_{(x_0 ,y_0 )} \Delta y \\ &+ \frac{1}{2!} \left( \eval{\frac{\partial^2 f}{\partial x^2}}_{(x_0 ,y_0 )}  (\Delta x)^2 + 2\eval{\frac{\partial^2 f}{\partial x \partial y}}_{(x_0 ,y_0 )}(\Delta x)(\Delta y) + \eval{\frac{\partial^2 f}{\partial y^2}}_{(x_0 ,y_0 )} (\Delta y)^2 \right) \\ &+ \mathcal{O}((\Delta x)^3 ) + \mathcal{O}((\Delta y)^3 ).        
    \end{aligned}
\end{equation}

It can be shown that the general Taylor's theorem can be written as 

\begin{equation}
    f(\vb{x} ) = \sum_{n=0}^{\infty} \frac{1}{n!} \eval{\left[ (\Delta \vb{x} \cdot \nabla )^{n} f(\vb{x} ) \right]}_{\vb{x} = \vb{x} _{0}}  .
\end{equation}

\section{Speical Points of a Function}

From the Taylor's series above, we can see that a necessary and sufficient condition for a stationary point is that both partial derivatives vanish

\begin{equation}
    \eval{\frac{\partial f}{\partial x}}_{(x_0 ,y_0 )} = \eval{\frac{\partial f}{\partial y}}_{(x_0 ,y_0 )} =0.
\end{equation}

To find the natures of the sationary points, we first complete the square so that 

\begin{equation}
    df = f(x,y) - f(x_0 , y_0 ) \approx  \frac{1}{2} \left[ f_{xx}\left( \Delta x+ \frac{f_{xy}\Delta y }{f_{xx} }  \right)^2 + (\Delta y)^2\left( f_{yy} - \frac{f_{xy}^2 }{f_{xx} }   \right)  \right].
\end{equation}

For a minimum point, we require that \(df>0\) for arbitrary \(\Delta x \text { and } \Delta y\). This implies that \(f_{xx} >0 \text { and }  f_{xx}f_{yy} > f_{xy}^2\). Due to symmetry of \(x \text { and } y\), \(f_{y y } >0\) is also necessary. For saddle point, \(df\) can be positve, negative or zero depending on the choice of \(\Delta x \text { and } \Delta y\). Therefore,

\begin{enumerate}
    \item Minimum point: \(f_{xx} > 0, f_{yy} > 0 \text { and } f_{xy}^2 < f_{xx}f_{yy}\).
    \item Maximum point: \(f_{xx} < 0, f_{yy} < 0 \text { and } f_{xy}^2 < f_{xx}f_{yy}\).
    \item Saddle point: \(f_{xy}^2 > f_{xx}f_{yy}\). 
\end{enumerate}

If \(f_{xy}^2 = f_{xx}f_{yy}\), then \(df\) must be one of the four forms \(\pm  (\abs{f_{xx} }^{\frac{1}{2} } \Delta x /2 \pm \abs{f_{yy} }^{\frac{1}{2} }\Delta y)^2\), then for some choice of the ratio \(\Delta y / \Delta x\), \(df = 0\) so higher order terms are needed to find the nature of the stationary point. 

For functions with more than 2 variables, the conditions for stationary points are 

\begin{equation}
    \frac{\partial f}{\partial x_{i} } = 0 \text{ for all }  x_{i},
\end{equation}

where \(x_{i} \) are the variables. 

To investigate the nature of the stationary points, we again use the second order term of the Taylor's series

\begin{equation}
    df = f(\vb{x} ) - f(\vb{x} _{0} ) \approx \frac{1}{2}\sum_{i} \sum_{j} \frac{\partial^2 f}{\partial x \partial y} \Delta x_{i}\Delta x_{j},      
\end{equation}

which must be positive for all \(\Delta x_{i} \).

\chapter{Vector Calculus}

\section{Curvilinear coordinates}

\subsection{General Curvilinear Coordinates}

A point in three-dimensional space can be specified by three coordinates \((u,v,w)\). In Cartesian coordinates, \((u,v,w) = (x,y,z)\); In spherical coordinates, \((u,v,w) = (r, \theta, \phi)\); In cyilndrical coordintes, \((u,v,w) = (\rho, \phi, z)\). 

The infinitesimal displacement vector \(d \vb{r} \)  from \((u,v,w)\) to \((u+du, v+dv, w+dw)\) can be written as

\begin{equation} 
	d\vb{r} = \frac{\partial \vb{r} }{\partial u} du + \frac{\partial \vb{r} }{\partial v}dv + \frac{\partial \vb{r} }{\partial w}dw. 
\end{equation}

If the coordinate system is orthogonal \textit{i.e.,} \(\vu{u} \perp \vu{v} \perp \vu{w}\), where \(\vu{u} ,\vu{v} \text { and } \vu{w} \) are the unit vectors whose direction are directed along increaseing \(u, v \text { and } w\) respectively, then we have

\begin{equation}
    \frac{\partial \vb{r} }{\partial u} = f \vu{u}, \frac{\partial \vb{r} }{\partial v} = g \vu{v}  \text { and } \frac{\partial \vb{r} }{\partial w} = h \vu{w}  ,
\end{equation}

where \(f,g\) and \(h\) are characteristic constants of a coordinates system which scale the unit vectors. In Cartesian coordinates, \((f,g,h) = (1,1,1)\); In spherical coordinates, \((f,g,h) = (1,r,r\sin{\theta})\); In cyilndrical coordinates, \((f,g,h) = (1,\rho ,1)\).
	
The infinitesimal displacement vector is now 

\begin{equation}
    d \vb{r} = f(du\vu{u}) + g(dv\vu{v}) + h(dw\vu{w}) \label{dl} 
\end{equation}

and the arc length is the norm of \(d \vb{r} \), which is 

\begin{equation}
    ds = \sqrt{d \vb{r} \cdot d \vb{r} } = \sqrt{(fdu)^2 + (gdv)^2 + (hdw)^2}.  
\end{equation}



The infinitesimal area perpendicular to \(\vu{w}\) will be a rectangle with area
\begin{equation}
	dS = (fg)dudv \label{da}
\end{equation} 

as shown in \cref{infloop}.

\onefig{infloop}{scale=0.3}
	
The infinitesimal volume is a parallelepiped (rectangular solid if the system is orthogonal) with volume

\begin{equation}
    d \tau = \abs{fd \vu{u} \cdot (gd \vu{v} \cross hd \vu{w} )}dudvdw = (fgh)dudvdw 
\end{equation}

as shown in \cref{infvol}. 

\onefig{infvol}{scale=0.3}
	
\subsection{Spherical Coordinates}
\onefig{spherical}{scale=0.7}


From \cref{spherical}, the relations of the two set of variables \((x,y,z) \text { and } (r,\theta ,\phi  )\)  are

\begin{equation}
    \begin{cases} x = r\sin \theta \cos \phi \\ y = r\sin \theta \sin \phi \\ z = r\cos \theta \end{cases} \text { or } \begin{cases} r = \sqrt{x^2+y^2+z^2} \\  \theta = \arctan {\left(\sqrt{x^2+y^2}/z   \right)} \\  \phi = \arctan {\left(y /x\right)} \end{cases}. 
\end{equation}

A general position vector can then be written in 4 different ways as 

\begin{equation}
    \vb{r} = x \vu{i} + y \vu{j} + z \vu{k} = r\sin \theta \cos \phi \vu{i} + r\sin \theta \sin \phi \vu{j} + r \cos \theta \vu{k} = r \vu{r} = \sqrt{x^2+y^2+z^2}\vu{r}, 
\end{equation}

where the last one is seldom used.

A general infinitismal displacement vector can be written as 

\begin{equation}
    \begin{aligned}
    d \vb{r} &= \frac{\partial \vb{r} }{\partial x} dx + \frac{\partial \vb{r} }{\partial y} dy + \frac{\partial \vb{r} }{\partial z} dz = dx \vu{i} + dy \vu{j} + dz \vu{k} \\ &= \frac{\partial \vb{r} }{\partial r} dr + \frac{\partial \vb{r} }{\partial \theta }d\theta + \frac{\partial \vb{r} }{\partial \phi }d \phi =f dr \vu{r} +g d\theta \vu{\boldsymbol{\theta } } +h  d \phi \vu{\boldsymbol{\phi } }. 
    \end{aligned}
\end{equation}

where \(\partial \vb{r} /\partial r, \partial \vb{r} /\partial \theta \text { and } \partial \vb{r} /\partial \phi \) can be found by direct differentiaion as 

\begin{equation}
	\begin{aligned} 
		\frac{\partial \vb{r} }{\partial r}   &= f \vu{r} = \sin{\theta}\cos{\phi}\vu{i} + \sin{\theta}\sin{\phi}\vu{j} + \cos{\theta}\vu{k}, \\
		\frac{\partial \vb{r} }{\partial \theta }  &= g \vu{\boldsymbol{\theta }} = r(\cos{\theta}\cos{\phi}\vu{i} + \cos{\theta}\sin{\phi}\vu{j} - \sin{\theta}\vu{k})\text { and }  \\
		\frac{\partial \vb{r} }{\partial \phi }  &= r \vu{\boldsymbol{\phi } } = \sin \theta (-\sin{\phi}\vu{i} + \cos{\phi}\vu{j}). 
	\end{aligned} 
\end{equation}

Thus \(f = 1, g = r \text { and }  h = r\sin \theta \).

We can thus solve for \((\vu{r} , \vu{\boldsymbol{\theta } }, \vu{\boldsymbol{\phi } })\) in terms of \((\vu{i}, \vu{j} ,\vu{k} )\) as 

\begin{equation}
    \begin{cases}
        \vu{r} = \sin \theta \cos \phi  \vu{i}  + \sin \theta \sin \phi  \vu{j}  + \cos \theta  \vu{k},  \\
        \vu{\boldsymbol{\theta } } = \cos \theta \cos \phi \vu{i}  + \cos \theta \sin \phi \vu{j} 0 \sin \theta \vu{k}, \\
        \vu{\boldsymbol{\phi } } = -\sin \phi  \vu{i} + \cos \phi \vu{j}. 
    \end{cases}
\end{equation}



We can also solve for \((\vu{i} , \vu{j}, \vu{k} )\) in terms of \((\vu{r} , \vu{\boldsymbol{\theta } }, \vu{\boldsymbol{\phi } } )\) as  

\begin{equation}
    \begin{cases}
        \vu{i} = \sin \theta \cos \phi \vu{r} + \cos \theta \cos \phi \vu{\boldsymbol{\theta } } - \sin \phi \vu{\boldsymbol{\phi } }, \\
        \vu{j} = \sin \theta \sin \phi \vu{r} + \cos \theta \sin \phi \vu{\boldsymbol{\theta } } + \cos \phi \vu{\boldsymbol{\phi } }, \\
        \vu{k} = \cos \theta \vu{r} - \sin \theta \vu{\boldsymbol{\theta } },
    \end{cases}
\end{equation}

which are seldomly used.



	
\subsection{Cylindrical Coordinates}
\onefig{cylindrical}{scale=0.5}

From \cref{cylindrical}, the relations of the two set of variables \((x,y,z) \text { and } (\rho , \phi , z)\)  are

\begin{equation}
    \begin{cases} x = \rho \cos \phi \\ y = \rho \sin \phi \\ z = z \end{cases} \text { or } \begin{cases} \rho  = \sqrt{x^2+y^2} \\  \phi = \arctan {\left( y /x  \right)} \\ z = z \end{cases}. 
\end{equation}

A general position vector can be written in 4 ways as

\begin{equation}
    \vb{r} = x \vu{i} + y \vu{j} + z \vu{k} = \rho \cos \phi  \vu{i} + \rho \sin \phi \vu{j} + \vu{k} = \rho \vu{\boldsymbol{\rho } } + z \vu{z} = \sqrt{x^2+y^2}\vu{\boldsymbol{\rho } } + z \vu{z} , 
\end{equation}

where the last one is seldom used.

A general infinitismal displacement vector can be written as 

\begin{equation}
    \begin{aligned}
    d \vb{r} &= \frac{\partial \vb{r} }{\partial x} dx + \frac{\partial \vb{r} }{\partial y} dy + \frac{\partial \vb{r} }{\partial z} dz = dx \vu{i} + dy \vu{j} + dz \vu{k} \\ &= \frac{\partial \vb{r} }{\partial \rho } d\rho  + \frac{\partial \vb{r} }{\partial \phi  }d \phi  + \frac{\partial \vb{r} }{\partial z}d z =f d \rho  \vu{\boldsymbol{\rho } }  +g d\phi \vu{\boldsymbol{\phi  } } +h  d z\vu{\boldsymbol{z} }. 
    \end{aligned}
\end{equation}

where \(\partial \vb{r} / \partial \rho , \partial \vb{r} / \partial \phi  \text { and } \partial \vb{r} /\partial z  \) can be found by direct differentiaion as 

\begin{equation}
	\begin{aligned} 
		\frac{\partial \vb{r} }{\partial \rho }  &= f \vu{\boldsymbol{\rho } } = \cos{\phi}\vu{i} + \sin{\phi}\vu{j}, \\
		\frac{\partial \vb{r} }{\partial \phi }  &= g \vu{\boldsymbol{\phi } } = \rho (-\sin{\phi}\vu{i} + \cos{\phi}\vu{j}) \text { and }  \\
		\frac{\partial \vb{r} }{\partial z }  &= h \vu{k} = \vu{k}. 
	\end{aligned} 
\end{equation}

Thus \(f = 1, g = \rho  \text { and }  h = 1\). 

We can thus solve for \((\vu{\boldsymbol{\rho } }, \vu{\boldsymbol{\phi } }, \vu{k} )\) in terms of \((\vu{i} ,\vu{j} ,\vu{k} )\) as

\begin{equation}
    \begin{cases} 
        \vu{\boldsymbol{\rho } } = \cos \phi \vu{i} + \sin \phi \vu{j}, \\
        \vu{\boldsymbol{\phi } } = -\sin \phi \vu{i} + \cos \phi \vu{j}, \\
        \vu{k} = \vu{k}.
        \end{cases}
\end{equation}



We can also solve for \((\vu{i} , \vu{j}, \vu{k} )\) in terms of \((\vu{\boldsymbol{\rho } }, \vu{\boldsymbol{\phi } }, \vu{k} )\) as  

\begin{equation}
    \begin{cases}
        \vu{i} &= \cos \phi  \vu{\boldsymbol{\rho } } - \sin \phi \vu{\boldsymbol{\phi } }, \\
        \vu{j} &= \sin \phi \vu{\boldsymbol{\rho } } + \cos \phi \vu{\boldsymbol{\phi } }, \\
        \vu{k} &= \vu{k},
        \end{cases}
\end{equation}

which are seldomly used.











\section{Gradient, Divergence and Curl} 

We start by stating the gerneral form of del operator, gradient, divergence, curl, and Laplacian are defined as\footnote{The del operator is not written as \(\frac{1}{f} \frac{\partial}{\partial u}\vu{u}  + \frac{1}{g} \frac{\partial}{\partial v}\vu{v} + \frac{1}{h} \frac{\partial}{\partial w}\vu{w}\) because in a general coordinates system, unit vector is not a constant but depends on the coordinates of the point in space, so we take the unit vectors out of the partial derivatives to avoid confusion since we are not differentiating them.}

\begin{equation} \label{all} 
\begin{aligned}
\grad &= \frac{1}{f}\vu{u}\frac{\partial}{\partial u} + \frac{1}{g}\vu{v}\frac{\partial}{\partial v} + \frac{1}{h}\vu{w}\frac{\partial}{\partial w}, \\[10pt]
\grad{t }&= \frac{1}{f} \vu{u} \frac{\partial t}{\partial u} 
+ \frac{1}{g} \vu{v}\frac{\partial t}{\partial v}  
+ \frac{1}{h} \vu{w}\frac{\partial t}{\partial w} , \\[10pt]
\div{\vb{T}}  &= \frac{1}{fgh} \left[ 
\frac{\partial}{\partial u} (ghT_{u} ) + 
\frac{\partial}{\partial v} (fhT_{v} )+ 
\frac{\partial}{\partial w} (fgT_{h} ) \right], \\[10pt]
\curl{\vb{T} }  &= \frac{1}{fgh} 
\begin{vmatrix} 
f \vu{u} & g \vu{v} & h \vu{w} \\ 
\partial /\partial u & \partial /\partial v & \partial /\partial w \\ 
fT_{u}  & g T_{v}  & h T_{w}  
\end{vmatrix}, \\[10pt]
\laplacian t &= \div{(\grad{t} )} =  (1 / fgh) \left[
\frac{\partial}{\partial u} \left( \frac{gh}{f} \frac{\partial t}{\partial u} \right) + 
\frac{\partial}{\partial v} \left( \frac{fh}{g} \frac{\partial t}{\partial v} \right) + 
\frac{\partial}{\partial w} \left( \frac{fg}{h} \frac{\partial t}{\partial w} \right)
\right],
\end{aligned}
\end{equation}

where \(t (u,v,w)\) is a scalar field, which appoints each point in space a scalar, and \(\vb{T} = T_{u}\vu{u} + T_{v}\vu{v} + T_{w}\vu{w} \) is a vector field which appoints each point in space a vector.  

\subsection{Gradient}
Using the del operator defined in \cref{all}, we can write the infinitesimal change of a scalar function \(t(u,v,w)\) as

\begin{equation}
    dt = \pdv{t}{u}du + \pdv{t}{v}dv + \pdv{t}{w}dw = \grad{t} \cdot d\vb{r}  = \abs{\grad{t} }\abs{d \vb{r} }\cos \theta   . \label{dt} 
\end{equation}

where \(\theta\) is the angle between \(\grad{t}\) and \(d\vb{r}\).
	
From the above equation, it is evident that \(dt\) attains maximum when \(\theta = 0\), \textit{i.e.} \(d\vb{r} \parallelsum \grad{t}\). Thus, the gradient \(\grad{t}\) points in the direction of maximum increase of the function \(t\) and \(\abs{\grad{t}}\) gives the slope along this maximal direction.

%\example{Griffiths (5th ed.) Problem 1.12}
%{The height of a certain hill is given by \(h(x,y) = 10(2xy - 3x^2 - 4y^2 -18x + 28y +12)\). Find the location of the summit, and the magnitude and direction of the sleepest slope at \((1,1)\).}
%{The gradient is \(\grad{h} = 10((2y - 6x - 18)\vu{i} + (2x - 8y + 28)\vu{j})\). At the summit, \(\grad{h} = 0\). So, \(\begin{cases}
%	2y - 8x - 18 = 0  \\
%	2x - 8y + 28 = 0
%\end{cases}\), which gives \((x,y) = (3,2)\).
%\(\grad{h} = 220(-\vu{i} + \vu{j})\) at \((1,1)\), therefore the slope at \((1,1)\) equals to \(\abs{\grad{h}} = 220\sqrt{2}\) and the direction is northwest.}

\todo{problem 1.14, 1.17, 1.1.5}
	
Integrating, we get the fundamental theorem for gradients

\begin{equation} 
	t(\vb{b}) - t(\vb{a}) = \int_{\vb{a} }^{\vb{b} } dt = \int_{\vb{a}}^{\vb{b}} \grad{t} \cdot d\vb{r}. 
\end{equation}
	
This equation shows that if one would like to determine the height of Mountain Everest, one could place altimeters at the top and the bottom and subtract the two readings, or climb the mountain and measure the rise at each step.

A quick corollary is that if the vector field \(\vb{T} (\vb{r} )\) can be written as the gradient of a scalar field \(\grad{t}  \), then the line integral 

\begin{equation}
	\int_{\vb{a} }^{\vb{b} } \vb{T}(\vb{r} ) \cdot  d\vb{r}  = \int_{\vb{a} }^{\vb{b} } \grad{t} \cdot d\vb{r} = \int_{\vb{a} }^{\vb{b} }dt = t(\vb{b} ) - t(\vb{a} )
\end{equation}

is independent of \(\vb{a} \text { and } \vb{b} \) and the vector field \(\vb{T} (\vb{r} )\) is known as a conservative field. Then also, \(\oint_{C} \vb{T} (\vb{r} ) \cdot d\vb{r} = 0\) for any closed loop if \(\vb{T} (\vb{r} )\) is conservative, since \(t(\vb{b}) - t(\vb{a}) = 0\).\footnote{The converse, that we can always find a potential function \(t(\vb{r} )\) for \(\oint_{C}\vb{T} (\vb{r} )\cdot d\vb{r}  =0\) is also true, but involve \href{https:////www.damtp.cam.ac.uk//user//tong//vc//vc.pdf}{moderate algebra}, so we will take this obvious fact as granted.  } 

A quick way to check whether \(\vb{T} (\vb{r} )\) is conservative is to check if its curl is zero, since the curl of a gradient is always zero, so \(\curl{\vb{F} (\vb{r} )} = \curl{(\grad{t(\vb{r} )} )} =0 \).

Note that if \(\vb{T} (\vb{r} )\) is conservative then its total differential \(\vb{T} (\vb{r} ) \cdot d\vb{r} = \grad{t(\vb{r} )} \cdot d\vb{r} = dt \) is exact.  

An additional condition for us to safely say that \(\vb{T} (\vb{r} )\) is conservative, which is usually assumed to be met, is that \(t(\vb{r} )\) should be continuous. For example, for the field

\begin{equation}
	\vb{T} (\vb{r} ) = \left( -\frac{y}{x^2+y^2}, \frac{x}{x^2+y^2}   \right)
\end{equation}

zero curl, so the it is conservative and the corresonding potential function is 

\begin{equation}
	t(\vb{r} ) = \tan ^{-1} \left( \frac{y}{x}  \right).
\end{equation}

However, the loop integral 

\begin{equation}
	\oint_{C} \vb{T} (\vb{r} ) \cdot d\vb{r} = 2\pi \neq 0
\end{equation}

when \(C\) is a circle centered at the origin. More generally the loop integral is \(2\pi n\), where \(n\) is the number of times the loop winds around the origin. 

The inconsistency arises from the fact that \(t(\vb{r} )\) is not continuous at the origin, or in other words, \(\vb{T} (\vb{r} )\) is not continuously differentiable and fails to be defined at the origin, so \(\vb{T} (\vb{r} )\) is only conservative on the plane excluding the origin, \textit{i.e.,} \(\mathbb{R}^2 - \{0,0\}\). If the loop integral of \(\vb{T} (\vb{r} )\) includes the origin then it would not be zero, since \(\vb{T} (\vb{r} )\) is no longer conservative. 





It can also be shown that the gradient at a point can be written in terms of a surface integral over an infinitesimal surface surrounding the point

\begin{equation}
	\grad{t } = \lim_{V \to 0} \left( \frac{1}{V} \oint_{S} t  d\vb{S}  \right).
\end{equation}

However the proof is complicated and is not presented here.

\example{Gradient of \(\rcurs ^{-1} \) in Cartesian Coordinates.}
{Evaluate \(\grad({\rcurs ^{-1} })\), where \(\rcurs = \abs{\brcurs} = \abs{\vb{r} - \vb{r'}} = \abs{(x - x')\vu{i} + (y - y')\vu{j} + (z - z')\vu{k}} = \sqrt{(x-x')^2+(y-y')^2+(z-z')^2}\). Generalize the result to obtain \(\grad{(\rcurs^n)}\).}
{\begin{equation} 
	\begin{aligned} 
		\grad{\rcurs ^{-1} } &= \vu{i}\pdv{x}((x - x')^2 + (y - y')^2 + (z - z')^2)^{-\frac{1}{2}} \\ &+ \vu{j}\pdv{y}((x - x')^2 + (y - y')^2 + (z - z')^2)^{-\frac{1}{2}} \\ &+ \vu{k}\pdv{z}((x - x')^2 + (y - y')^2 + (z - z')^2)^{-\frac{1}{2}} \\ &= (-\frac{1}{2})(2)(((x - x')^2 + (y - y')^2 + (z - z')^2))^{-\frac{3}{2}} \\ &((x - x')\vu{i} + (y - y')\vu{j} + (z - z')\vu{k}) \\ &=- \brcurs \rcurs ^{-1} . 
	\end{aligned} 
\end{equation}
		
This result can be easily generalised to get
		
\begin{equation} 
	\grad{(\rcurs^n)} = n\rcurs^{n-1}\hrcurs. \label{gradrcurs} 
\end{equation}}

\example{Gradient in Cartesian and Polar Coordinates.}
{Calculate the gradient of the function \(f(x,y) = x^2+y^2\) in both Cartesian and the polar coordinates.}
{In Cartesian coordinates, the gradient is \(\boldsymbol{\nabla }f(x,y)   = (2x,2y) \). 

The two sets of variables \(\vb{x} = (x,y) \text { and } \vb{u} = (r,\theta )\) are related by \(x = r\cos \theta \text { and } y = r\sin \theta \). The gradients are related by \cref{jacobderi}, as

\begin{equation}
	\begin{pmatrix}
		 \frac{\partial }{\partial r}  \\
		 \frac{\partial }{\partial \theta }  \\
	\end{pmatrix}  = \begin{pmatrix}
		\frac{\partial x}{\partial r}  & \frac{\partial y}{\partial r}   \\
		\frac{\partial x}{\partial \theta }  & \frac{\partial y}{\partial \theta }   \\
	\end{pmatrix} \begin{pmatrix}
		 \frac{\partial }{\partial x}  \\
		 \frac{\partial }{\partial y}  \\
	\end{pmatrix}  = \begin{pmatrix}
		\cos \theta  & \sin \theta   \\
		-r\sin \theta  & r\cos \theta   \\
	\end{pmatrix} \begin{pmatrix}
		 \frac{\partial }{\partial x}  \\
		 \frac{\partial }{\partial y}  \\
	\end{pmatrix}.
\end{equation}

Inverting the equations, we have

\begin{equation}
	\begin{pmatrix}
		 \frac{\partial }{\partial x}  \\
		 \frac{\partial }{\partial y}  \\
	\end{pmatrix} = \begin{pmatrix}
		\cos \theta  & -\frac{\sin \theta }{r}   \\
		\sin \theta  & \frac{\cos \theta }{r}   \\
	\end{pmatrix} \begin{pmatrix}
		 \frac{\partial }{\partial r}  \\
		 \frac{\partial }{\partial \theta }  \\
	\end{pmatrix} = \begin{pmatrix}
		 \cos \theta \frac{\partial }{\partial r} - \frac{\sin \theta }{r}\frac{\partial }{\partial \theta }   \\
		 \sin \theta \frac{\partial }{\partial r} + \frac{\cos \theta }{r}\frac{\partial }{\partial \theta }   \\
	\end{pmatrix}.
\end{equation}


Therefore, we have 

\begin{equation}
	\begin{aligned} 
	\boldsymbol{\nabla }f(x,y)  &= \left( \cos \theta \frac{\partial f}{\partial r} - \frac{\sin \theta}{r} \frac{\partial f}{\partial \theta }   \right) \vu{i} + \left( \sin \theta \frac{\partial f}{\partial r} + \frac{\cos \theta }{r} \frac{\partial f}{\partial \theta } \right)\vu{j} = 2r(\cos \theta , \sin \theta ) \\ 
	&= \frac{\partial f}{\partial r} \vu{r} + \frac{1}{r} \frac{\partial f}{\partial \theta }\vu{\boldsymbol{\theta } } = 2r \vu{r} = 2r(1,0).  
	\end{aligned}   
\end{equation}

Note that the answer \(2r(\cos \theta ,\sin \theta )\) is still in Cartesian coordiantes, just that we used variables in polar coordinates to represent it. The answer \(2r(1,0)\) on the other hand, is the gradient in polar coordiantes.   



} 

\example{Gradient to Compute Changes.}
{Consider the function \(T\) in the \(x\)-\(y\) plane, given by \(T = \ln (x^2+y^2)\). At point \((1,2)\),

\begin{enumerate}
	\item in which direction is the increase in \(T\) most rapid?
	\item what distance in the direction found in (a) gives an increase of 0.2?
	\item what distance in the direction of \(\vu{i} + \vu{j} \) gives an increase of 0.12?
	\item in which directions will \(T\) be stationary? 
\end{enumerate}
~
}
{\begin{enumerate}
	\item The direction is given by its gradient \(\grad{T} = (2/ 5, 4 /5) \).
	\item The change in \(T\) is given by \(dT = \grad{T} \cdot d\vb{r} = \abs{\grad{T} }s = 0.2 \), so \(s = \sqrt{5} /10 \).  
	\item The change in \(T\) is given by \(dT = \grad{T} \cdot d\vb{r} =  \grad{T} \cdot (1,1) s /\sqrt{2} = 0.12\), so \(s = 4 \sqrt{5}/ 10 \).  
	\item We require \(dT = 0\), so \(d \vb{r} \perp \grad{T} \), so the direction is \(\pm (2,-1)\).   
\end{enumerate}
~
} 



The gradient in Cartesian and spherical coordinates are stated here for reference:

\begin{equation} 
	\grad{t} = \frac{\partial t}{\partial x} \vu{i} + \frac{\partial t}{\partial y} \vu{j} + \frac{\partial t}{\partial z} \vu{k} = \pdv{t}{r}\vu{r} + \frac{1}{r}\pdv{t}{\theta}\vu{\boldsymbol{\theta } } + \frac{1}{r\sin{\theta}}\pdv{t}{\phi}\vu{\boldsymbol{\phi } }. 
\end{equation}
	
\subsection{Divergence}

	
In order to seek geometrical interpretation of the divergence operator, we consider the closed surface integral of \(\vb{T}\) over the surface of an infinitesimal volume depicted in \cref{infvol} (here we adopt the sign convention of \(\vb{S}\) is positive if \(\vb{S}\) is pointing outwards from the interior of the volume)

\begin{equation} \label{divpre} 
	\begin{aligned} 
    \oint_{S} \vb{T} \cdot d\vb{S} &= ((T_u)|_{u+du} - (T_u)|_{u})(ghdvdw) \\ &+ ((T_v)|_{v+dv} - (T_v)|_{v})(fhdudw) \\ &+ ((T_w)|_{w+dw} - (T_w)|_{w})(fgdudv) \\
    &= \frac{1}{fgh} \left( 
		\frac{\partial}{\partial u} (ghT_{u} ) + 
		\frac{\partial}{\partial v} (fhT_{v} )+ 
		\frac{\partial}{\partial w} (fgT_{h} ) \right) d\tau \\
	&= (\div{\vb{T}}) d\tau
    \end{aligned} 
\end{equation} 
	
This result can be extended easily. As any arbitary volume can be divided infinitely into infinitesimal pieces, and the surface integral of each individual pieces cancel in pairs, the remaining part is only the surface integral of the surface of the whole volume. Therefore,
	
\begin{equation} 
	\oint_{S} \vb{T} \cdot d\vb{S} = \int_{V}(\div{\vb{T} }) d\tau \label{divthm}.  
\end{equation}
	
This is the divergence theorem (also known as the Gauss's theorem or the Green's theorem). From the LHS of \cref{divthm}, it is evident that the divergence of a vector function is a measure of how much the function spreds out and diverges from a given point in space.


If \(\vb{T} \) satisfies \(\div{\vb{T} } = 0 \), then \(\vb{T} = \curl{\vb{T} ' + \grad{t} + \vb{C} } \) can be written as a curl of a vector \(\vb{T} '\), which statisfies \(\curl{\vb{T} '} = 0 \) plus a gradient of a scalar function \(t\) plus a constant vector \(\vb{C} \).        

The divergence in Cartesian and spherical coordinates are stated here for reference:
\begin{equation} 
	\div{\vb{T}} = \pdv{T_x}{x} + \pdv{T_y}{y} + \pdv{T_z}{z} = \frac{1}{r^2}\pdv{r}(r^2T_r) + \frac{1}{r\sin{\theta}}\pdv{\theta}(\sin{\theta}T_{\theta}) + \frac{1}{r}\pdv{\phi}(rT_{\phi}). 
\end{equation}
	
\subsection{Curl}

In order to seek geometrical interpretation of the curl operator, we consider the loop integral of \(\vb{T}\) over an infinitesimal loop depicted in \cref{infloop} (here since the coordinates system is right handed, \(\vu{w}\) points out of the page, hence we are obliged by the right-hand rule to run the line integral counterclockwise such that \(\vb{S}\) points in the same direction as \(\vu{w}\))
	
\begin{equation} 
	\begin{aligned} 
		\oint_{C} \vb{T} \cdot d\vb{r} &= (T_u)|_{v}fdu + (T_v)|_{u+du}gdv + (T_u)|_{v+dv}(-fdu) + (T_v)|_{u}(-gdv) \\ &= \frac{1}{fg}\left(\frac{\partial }{\partial u} (T_{v} g) - \frac{\partial }{\partial v} (T_{u}f)\right)(\vu{w} \cdot d\vb{S}) \\ &= (\curl{\vb{T} } ) \cdot d\vb{S} . 
	\end{aligned} 
\end{equation}

With the same argument as \cref{divpre}  to \cref{divthm}, the above equation can be extended to
	
\begin{equation} 
	\oint_{C} \vb{T} \cdot d\vb{r} = \int_{S} (\curl{\vb{T}}) \cdot d\vb{S}. \label{stothm} 
\end{equation}
	
This is the Stoke's theorem. From the LHS of \cref{stothm}, it is evident that the curl of a vector function is a measure of how much the function rotates and curls around a given point in space.


A quick corollary is that the integral \(\int_{S} (\curl{\vb{T}}) \cdot d\vb{S}\) is independent of the surface used but only depends on the boundary line. Hence, \( \oint_{S} (\curl{\vb{T}}) \cdot d\vb{S} = 0\) for any closed surface, since the boundary line, like the mouth of a ballon, shrinks down to a point, and thus \(\oint_{P} \vb{T} \cdot d\vb{r} = 0\).

\(\vb{T} \) is called conservative if \(\int_{A}^{B}  \vb{T} \cdot d\vb{r}  \) is independent of the path taken from \(A\) to \(B\). This implies that \( \oint_{C} \vb{T} \cdot d\vb{r} = 0\) and thus \(\curl{\vb{T} } = 0\) and \(\vb{T} = \grad{t} \) can be written as a gradient of some scalar function \(t\). Also, \(\vb{T} \cdot d\vb{r} \) is an exact differential.      

The curl in Cartesian and spherical coordinates are stated here for reference:
\begin{equation} 
	\begin{aligned}
	\curl{\vb{T}} &= (\pdv{T_z}{y} - \pdv{T_y}{z})\vu{i} + (\pdv{T_x}{z} - \pdv{T_z}{x})\vu{j} + (\pdv{T_y}{x} - \pdv{T_x}{y})\vu{k} \\ &= \frac{1}{r\sin{\theta}}\left(\pdv{\theta}(\sin{\theta}T_{\phi}) - \pdv{T_{\theta}}{\phi}\right)\vu{r} + \frac{1}{r}\left(\frac{1}{\sin{\theta}}\pdv{T_r}{\phi} - \pdv{r}(rT_{\phi})\right)\vu{\boldsymbol{\theta } } + \frac{1}{r}\left(\pdv{r}(rT_{\theta}) - \pdv{T_r}{\theta}\right)\vu{\boldsymbol{\phi } }. 
    \end{aligned}
\end{equation}
	

\example{Curl.}
{Find the vector function \(\vb{G} \) such that \(\curl{\vb{G} } = \vb{F} \), given that \(\vb{F} = (xz^2,-yz^2,0)\).  }
{Since \(F_{z}= 0 \), a simple approach is to set \(G_{x} = G_{y} =0  \), then the curl reduces to 

\begin{equation}
	\curl{\vb{G} } = (\partial _{y}G_{z} ,- \partial _{x}G_{z}, 0    ). 
\end{equation}

So we need 

\begin{equation}
	\partial _{y}G_{z} = xz^2 ~\text { and }~ \partial _{x}G_{z} = yz^2.    
\end{equation}

Integrating the first equation gives

\begin{equation}
	G_{z} = xyz^2 + h(x,z).  
\end{equation}

Substituting into the second equation gives 

\begin{equation}
	\partial _{x}G_{z} = yz^2 + \frac{\partial h}{\partial x} = yz^2 \implies \frac{\partial h}{\partial x} = 0 \implies h = \text{constant}.   
\end{equation}

Therefore the \(\vb{G} = (0,0,xyz^2 + C)\). 
} 

\section{Product Rules}
The two product rules for gradient are

\begin{equation}
\begin{cases} 
	\grad{(fg)} = f\grad{g} + g\grad{f},  \\
	\grad{(\vb{A} \cdot \vb{B})} = \vb{A} \cross (\grad \cross \vb{B}) + \vb{B} \cross (\grad \cross \vb{A}) + (\vb{A} \cdot \grad)\vb{B} + (\vb{B} \cdot \grad)\vb{A}. 
\end{cases}
\end{equation}
	
The two product rules for divergence are 

\begin{equation}
\begin{cases} 
	\div{(f\vb{A})} = f(\div{\vb{A}}) + \vb{A} \cdot (\grad{f}), \\
	\div (\vb{A} \cross \vb{B}) = \vb{B} \cdot (\curl {\vb{A}}) - \vb{A} \cdot (\curl{\vb{B}}). 
\end{cases}
\end{equation}

	
The two product rules for curl are
	
\begin{equation}
\begin{cases} 
	\curl{(f\vb{A})} = f(\curl{\vb{A}}) - (\vb{A} \cross (\grad{f})), \\
	\curl (\vb{A} \cross \vb{B}) = (\vb{B} \cdot \grad)\vb{A} - (\vb{A} \cdot \grad)\vb{B} + \vb{A}(\div{\vb{A}}) - \vb{B}(\div{\vb{A}}). 
\end{cases}
\end{equation}

	
Here note that

\begin{equation} 
	\begin{aligned} 
		(\vb{A} \cdot \grad) \vb{B} &= \left(A_x\pdv{x} + A_y\pdv{y} + A_z\pdv{z}\right)\left(B_x\vu{i} + B_y\vu{j} + B_z\vu{k}\right) \\ &= \left(A_x\pdv{B_x}{x} + A_y\pdv{B_x}{y} + A_z\pdv{B_x}{z}\right)\vu{i} \\ &+ \left(A_x\pdv{B_y}{x} + A_y\pdv{B_y}{y} + A_z\pdv{B_y}{z}\right)\vu{j} \\ &+ \left(A_x\pdv{B_z}{x} + A_y\pdv{B_z}{y} + A_z\pdv{B_z}{z}\right)\vu{k} \\ &\neq \vb{A} \cdot (\grad{\vb{B}}) \\ &= A_x\pdv{B}{x} + A_y\pdv{B}{y} + A_z\pdv{B}{z} .
	\end{aligned} 
\end{equation}

In component form, this is \((\vb{A} \cdot \grad{} )_{i} = A_{i}\partial _{i}   \). Another differential operator we can construct is \((\vb{A} \cross  \grad{} )_{k} = \epsilon _{ijk} A_{i}\partial _{j}     \) but it is seen much less often. 
	
With the product rules in hand, we can perform the so-called ``Integration by part'' trick. For example, by integrating the first product rule of divergence and using the divergence theorem, we have
	
\begin{equation} 
	\begin{aligned}
	\int_{V} \div{(f\vb{T})} d\tau &= \int_{V} f(\div{\vb{T}}) d\tau + \int_{V} \vb{T} \cdot (\grad{f}) d\tau = \oint_{S} f\vb{T} \cdot d\vb{A} \\
	\implies \int_{V} f(\div{\vb{T}}) d\tau &= \oint_{S} (f\vb{T}) \cdot d\vb{A} - \int_{V} \vb{T} \cdot (\grad{f}) d\tau. 
	\end{aligned}
\end{equation}
	
Here we transform the integrand from the product of one function (\(f\)) and one derivative \(\div{\vb{T}}\) to another integrand of the product of one function that is orginally the derivative \((\vb{T})\) and one derivative which is orginally the function \((\grad{f})\), at a cost of a minus sign and a boundary term \(\left(\oint_{S} (f\vb{T}) \cdot d\vb{A}\right)\), just like integration by part in ordinary derivatives, where
	
\begin{equation} 
	\int_{a}^{b} f(\dv{g}{x})dx = -\int_{a}^{b} g\left(\dv{f}{x}\right)dx + \eval{\left(fg\right)}_a^b 
\end{equation}
	
comes from the product rule
	
\begin{equation} 
	\dv{x}(fg) = f\left(\dv{g}{x}\right) + g\left(\dv{f}{x}\right). 
\end{equation}

Similarly, we can show that 
	
\begin{equation} 
	\int_{S} f(\curl{\vb{T}}) \cdot d\vb{A} = \int_{S} (\vb{T} \cross (\grad{f})) \cdot d\vb{A} + \oint_{C} f\vb{T} \cdot d\vb{r} 
\end{equation}
	
and

\begin{equation} 
	\int_{V} \vb{B} \cdot (\curl{\vb{T}}) d\tau = \int_{V} \vb{T} \cdot (\curl{\vb{B}}) d\tau + \oint_{S} (\vb{T} \cross \vb{B}) \cdot d\vb{A}. 
\end{equation}
	
\section{Second Derivatives}
	
From the nature of gradient, divergence and curl, we can construct five species of second derivatives. They are 
\begin{enumerate}
	\item Divergence of gradient \(\div{(\grad{t})}\):
		
	\begin{equation} 
		\div{(\grad{t})} = \left(\vu{i}\pdv{x} + \vu{j}\pdv{y} + \vu{k}\pdv{z}\right) \cdot \left(\vu{i}\pdv{t}{x} + \vu{j}\pdv{t}{y} + \vu{k}\pdv{t}{z}\right) = \pdv[2]{t}{x} + \pdv[2]{t}{y} + \pdv[2]{t}{z}, \label{prelap} 
	\end{equation}
		
	In fact, the divergence of gradient operator is so frequently used in physics that it gets its own name and symbol known as the Laplacian:
		
	\begin{equation}
		\laplacian{t} = \div{(\grad{t})}. \label{lap} 
	\end{equation}
		
	With reference to \cref{lap}, the Laplacian of a scalar function in spherical coordinates is listed here for reference:
	
	\begin{equation} 
		\laplacian{t} = \frac{1}{r^2}\pdv{r}(r^2\pdv{t}{r}) + \frac{1}{r^2\sin{\theta}}\pdv{\theta}(\sin{\theta}\pdv{t}{\theta}) + \frac{1}{r^2\sin{\phi}}\pdv[2]{t}{\phi}. 
	\end{equation}

	In some case, we can simplify the expression by rewriting the fisrt term on the RHS as 

	\begin{equation}
		\frac{1}{r^2} \frac{\partial }{\partial r} \left( r^2 \frac{\partial t}{\partial r}  \right) = \frac{1}{r} \frac{\partial^2 }{\partial r^2} (rt).   
	\end{equation}
	
	
		
	\item Curl of gradient:
		
	\begin{equation} 
		\curl{(\grad{t})} = 
		\begin{vmatrix}
			\vu{i}  & \vu{j}  & \vu{k}   \\
			\frac{\partial }{\partial x}  & \frac{\partial }{\partial y}  & \frac{\partial }{\partial z}   \\
			\frac{\partial t}{\partial x} & \frac{\partial t}{\partial y}  & \frac{\partial t}{\partial z}   \\
		\end{vmatrix} = \left(\pdv{t}{z}{y} - \pdv{t}{y}{z}\right)\vu{i} + \left(\pdv{t}{x}{z} - \pdv{t}{z}{x}\right)\vu{j} + \left(\pdv{t}{y}{x} - \pdv{t}{x}{y}\right)\vu{k} = 0. \label{curlgrad} 
	\end{equation}
		
	\item Gradient of divergence (not identical to the divergence of gradient):
		
	\begin{equation} 
		\grad{(\div{\vb{T}})} \neq (\div{\grad{} } )\vb{T}. 
	\end{equation}
		
	\item Divergence of curl:
		
	\begin{equation} 
		\div{(\curl{\vb{T}})} = \pdv{x}(\pdv{T_z}{y} - \pdv{T_y}{z}) + \pdv{z}(\pdv{T_x}{z} - \pdv{T_z}{x}) + \pdv{z}(\pdv{T_y}{x} - \pdv{T_x}{y}) = 0. \label{divcurl} 
	\end{equation}
		
	\item Curl of curl:
		
	\begin{equation} 
		\curl{(\curl{\vb{T}})} = \grad{(\div{\vb{T}})} - \laplacian{\vb{T}}. \label{curlcurl} 
	\end{equation} 
		
	Here, \(\laplacian{\vb{T}}\) is the Laplacian of a vector function defined as\footnote{In fact, \cref{curlcurl} is often used to define the Laplacian of a vector, since \cref{lap} makes explicit reference to Cartesian coordinates.}
		
	\begin{equation} 
		\laplacian{\vb{T}} = (\laplacian{T_x})\vu{i} + (\laplacian{T_y})\vu{j} + (\laplacian{T_z})\vu{k}. 
	\end{equation}	
\end{enumerate}	
	
For the 5 second derivatives listed above, only \cref{prelap,curlcurl} are used regularly in physics enough that they are worth remembering. All of the identities can be easily proven in index notation, as long as one remembers the basic rules 

\begin{equation}
	(\grad{t} )_{i} = \partial _{i}t, \quad (\div{\vb{T} } )_{i} = \partial _{i}T_{i} ~\text { and }~ (\curl{\vb{T} } )_{i} = \epsilon _{ijk}\partial _{j}T_{k}.          
\end{equation}

To find \(\grad{(r^{p}) } \), for example, we have 

\begin{equation}
	\partial _{i}r^{p} = \partial _{i}(x_{j}x_{j}  )^{\frac{p}{2} } = \frac{p}{2}(x_{j}x_{j})^{\frac{p}{2}-1 } \partial _{i}(x_{j}x_{j}  ) = pr^{p-2}x_{i} \implies \grad{r^{p} } = pr^{p-1}\vu{r} .           
\end{equation}

To prove that \(\curl{\grad{t} } = 0\), for example, we can show that 

\begin{equation}
	(\curl{\grad{t} })_{i}  = \epsilon _{ijk} \partial _{i}\partial _{j}t = 0,  
\end{equation}

since the \(\epsilon _{ijk} \) is antisymmetric over \(ij\), but the partial derivatives \(\partial _{i}\partial _{j}  \) are symmetric, so the terms like \(\partial _{1}\partial _{2} - \partial _{2}\partial _{1}    \)  cancel.   

\example{Index Notation (1).}
{Simplify \(\curl{(\varphi \vb{F} )} \). }
{We have 

\begin{equation}
	(\curl{\varphi \vb{F} } )_{i} = \epsilon _{ijk}\partial _{j}(\varphi \vb{F} )_{k}  = \epsilon _{ijk}(F_{k} (\partial _{j}\varphi  ) + \varphi (\partial _{j}F_{k} )) = \left( \grad{\varphi } \cross \vb{F}   \right)_{i} + \varphi (\curl{\vb{F} })_{i}.   
\end{equation}

} 

\example{Index Notation (2).}
{Simplify \(\curl{(\boldsymbol{\omega } \cross \vb{r}  )} \), where \(\boldsymbol{\omega } = \omega \vu{z} \).  }
{We have 

\begin{equation}
	\begin{aligned} 
	(\curl{(\boldsymbol{\omega }\cross \vb{r}  )} ) &= \epsilon _{ilm} \partial _{l} \epsilon _{mjk} \omega _{j} r_{k} = \epsilon _{ilm} \epsilon _{mjk} \omega _{j} \partial _{l} r_{k} \\
	&= \epsilon _{ilm} \epsilon _{mjk}  \omega _{j} \delta _{lk}  = (\delta _{ij} \delta _{kk} - \delta _{ik} \delta _{jk}   ) \omega _{j} = (3 \delta _{ij} - \delta _{ij}  ) \omega _{j} = \omega _{i}.
	\end{aligned}            
\end{equation}

Note that from the first equality to the second we have pulled \(\omega _{j} \) out of \(\partial _{l} \) since it is a constant and does not depends on position.  

} 



	
\section{Dirac Delta Function}
	
The motivation of the dirac delta function comes from the result of the divergence of the vector function \( \vb{T} = \vu{r}  /r^2\):
	
\begin{equation} 
	\div{\left(\frac{\vu{r}}{r^2}\right)} = \frac{1}{r^2} \pdv{r}(r^2\left(\frac{1}{r^2}\right)) = 0, \label{notmatchone} 
\end{equation}
	
which is not consistent with the result we would expect from the divergence theorem, since the suface integral over a hypothetical sphere with radius \(R\) is 	

\begin{equation}
	\oint_{S} \left( \frac{\vu{r} }{R^2} \right) \cdot  (R^2\sin \theta d \theta d \phi \vu{r} )  = 4\pi .
\end{equation}


	
The root of this problem lies on the fact that the function itself blows up at the origin, so while it is true that the divergence of this function equals to 0 at every point, it does not apply to the origin. The \(4\pi\) contribution in comes entirely from the orgin.
	
To describe this behaviour, we introduce the dirac delta function which is defined by
	
\begin{equation} 
	\delta(x) = \begin{cases} \infty &\text{if} ~~~ x=0 \\ ~0 &\text{if} ~~~ x\neq0 \end{cases} ~\text { and }~ 	\int_{-\infty}^{+\infty} \delta(x) dx = 1. \label{dd2} \footnote{Therefore, \(\delta(x)\) has the dimension of the inverse of its argument} 
\end{equation}	
	
The appearance of this function is shown in \cref{dd}.\onefig{dd}{scale = 0.7}
	
Let \(f(x)\) to be an ordinary (continuous) function, then it follows that

\begin{equation} 
	f(x)\delta(x) = f(0)\delta(x), \label{ddimport} 
\end{equation}
	
since \(f(x) \delta(x) \neq 0\) only if \(x = 0\).
	
Integrating the above equation,

\begin{equation} 
	\int_{-\infty}^{+\infty} f(x) \delta(x) dx = \int_{-\infty}^{+\infty} f(0) \delta(x) dx = f(0) \int_{-\infty}^{+\infty} \delta(x) dx = f(0). \label{pickout} 
\end{equation} 
	
Using the integral, the dirac delta function picks out the value of \(f(x)\) at the origin.
	
By changing the variable in \cref{pickout}, we can pick out the value of \(f(x)\) at any arbitary point \(x = a\)
	
\begin{equation} 
	\int_{-\infty}^{+\infty} f(x) \delta(x-a) dx = f(a) 
\end{equation}
	
\example
{Dirac Delta Function (1).}
{Show that \(\delta(kx) = \frac{1}{\abs{k}}\delta(x)\).}
{Let \(u = kx\), then 
			
\begin{equation} 
	\int_{-\infty}^{+\infty} f(x) \delta(kx) dx = \frac{1}{\abs{k} }  \int_{-\infty}^{+\infty} f\left(\frac{u}{k}\right) \delta(u) du = \frac{1}{\abs{k}} f(0) = \int_{-\infty}^{+\infty} f(x) \frac{\delta(x)}{\abs{k}} dx.
\end{equation}
			
The absolute signs are necessary since if \(k\) is negative then the lower and the upper limit of integration will be reversed. By comparing the integrands, we yield the desired result.}	
		
\example{Dirac Delta Function (2).}{Show that \(-\delta(x) = x\dv{x}(\delta(x))\)}

\example{Dirac Delta Function (3).}{Show that \(\delta(x) = d \theta /dx\), where \(\theta(x)\) is the Heaviside step function defined as \(\theta(x) = \begin{cases} 0 ~~~ \text{if} ~~ x~\le~0, \\  1 ~~~ \text{if} ~~ x~>~0. \end{cases}\)}

It is natural to generalize \(\delta(x)\) to three dimensions as follows:
	
\begin{equation} 
	\delta(\vb{r}) = \delta(x)\delta(y)\delta(z) 
\end{equation}
	
The characteristic equations the three-dimensional delta function are then 
	

\begin{equation} 
	\int_{\text{all space} } \delta^3(\vb{r})
	d\tau = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} \delta(x)\delta(y)\delta(z) ~dxdydz = 1 
\end{equation}
	
and
	
\begin{equation} 
	\int_{\text{all space} } f(\vb{r}) \delta^3(\vb{r} - \vb{a}) = f(\vb{a}). 
\end{equation} 
	
As in the one-dimensional case, the \(\delta^3(\vb{r} - \vb{a})\) picks out the value of \(f(\vb{r})\) at \(\vb{a}\).
	
The inconsistency can now be resolved as the divergence of the vector function \(\vb{T} = \vu{r} /r^2\) is in fact
	
\begin{equation} 
	\div{\left(\frac{\vu{r}}{r^2}\right)} = 4\pi\delta^3(\vb{r}). 
\end{equation}
	 
which equals to 0 except in the origin where it equals to \(4\pi\).
	
In general,
	
\begin{equation} 
	\div{\left(\frac{\hrcurs}{\rcurs^2}\right)} = 4\pi\delta^3(\brcurs). \label{divdelta} 
\end{equation} 
 	
Here, the derivative is evaluated with respect to \(\vb{r}\) and \(\vb{r'}\) is fixed.
 	
From \cref{gradrcurs}, we have also
 	
\begin{equation} 
	\laplacian{\left(\frac{1}{\rcurs}\right)} = -4\pi\delta^3(\brcurs). \label{laprj} 
\end{equation}

With this equation in hand, we can find prove that the potential created by the charge distribution \(\rho (\vb{r} ')\) satisfies the Poisson's equation 

\begin{equation}
	\begin{aligned} 
	\laplacian V(\vb{r} ) &= \laplacian  \left( \frac{1}{4\pi \epsilon_0} \int \frac{\rho (\vb{r} ')}{\rcurs }d\tau '  \right) = \frac{1}{4\pi \epsilon_0} \int \rho (\vb{r} ') \laplacian \left( \frac{1}{\rcurs }  \right) d\tau' \\
	&= \frac{1}{4\pi \epsilon_0} \int \rho (\vb{r} ')(-4\pi \delta ^3 (\brcurs ))d\tau ' = -\frac{\rho (\vb{r} )}{\epsilon_0 }.   
	\end{aligned} 
\end{equation}


	
\example{Griffith (5th ed.) Problem 1.47}
{Find the charge density \(\rho(\vb{r})\) for
\begin{enumerate}
	\item a point charge  \(q\) located at \(\vb{r'}\),\\
	\item an electric dipole consisting of a point charge \(-q\) at the origin and \(+q\) at \(\vb{a}\) and\\
	\item an infinitesimal sphere of charge \(q\) and radius \(a\) centered at the origin
\end{enumerate}~}
{\begin{enumerate}
	\item \(\rho(\vb{r}) = q\delta(\vb{r} - \vb{r'}) \)\\

	\item \(\rho(\vb{r}) = -q\delta(\vb{r}) + q\delta(\vb{r} - \vb{a})\)\\	

	\item \(\rho(r) = A\delta(r - a)\), where \(A\) can be determined by \(\int_{V} \rho(r) d\tau = \int A\delta(r-a) 4\pi r^2 dr = 4\pi Aa = q\), thus \(A = q /4\pi a\). Therefore, \(\rho(r) = q\delta(r - a) /4\pi a\).
\end{enumerate}~}

\example{Vortex Flow.}
{Consider vortex flow in an incompressible fluid with a velocity field

\begin{equation}
	\vb{v} = \frac{1}{\rho } \vu{\boldsymbol{\phi } }. 
\end{equation}

Explain the discrepancy in Stokes' theorem using Dirac delta function.
}
{From this velocity field \(\curl{\vb{v} } =0\) everywhere except on the axis \(\rho =0\) where \(\vb{v}\) has a singularity. Therefore \( \oint_{C} \vb{v} \cdot d\vb{r} =0\) for any path \(C\) that does not enclose the vortex line on the axis and \(2\pi \) if \(C\) does enclose the axis.

In order for Stokes' theorem to be valid for all paths \(C\), we therefore set 

\begin{equation}
	\curl{\vb{v} } = 2\pi \delta (\rho ).
\end{equation}

So \(\vb{v} \) is locally conservative over any region that excludess the axis, but is globally non-conservative. 

} 


	
\section{Helmholtz Theorem}

\begin{theorem}[Helmoholtz Theorem]
Given the divergence and curl of a differentiable vector function \(\vb{T}(\vb{r} ) \), it can be written as some gradient of some scalar function \(\Phi(\vb{r} ) \) plus some vector function \(\vb{A}(\vb{r} ) \) if \(\vb{T} \) goes to zero faster than \(1 /r \) as \(r \to \infty\), where \(\Phi \text { and } \vb{A} \) are given by

\begin{equation}
	\Phi(\vb{r} ) = \frac{1}{4\pi } \int_{\text{all space} } \frac{\boldsymbol{\nabla}' \cdot \vb{T} (\vb{r} ')}{\rcurs } d\tau ' + C~\text { and }~ \vb{A} (\vb{r} ) = \frac{1}{4\pi } \int_{\text{all space} } \frac{\boldsymbol{\nabla}' \cross \vb{T} (\vb{r} ')}{\rcurs } d\tau ' + \vb{C} + \grad{D} .
\end{equation}

Here \(C \text { and }  \vb{C} \) are some constant scalar and vector functions respectively, which does not change \(\vb{T} = \grad{\Phi } + \curl{\vb{A} }\), since the gradient and curl of some constant function is zero. \(\vb{D}\), on the other hand, can be any function since the curl of the gradient is always zero. 

\end{theorem}

\begin{proof}

We start from the existence theorem of the Poisson's equation\footnote{The proof of this theorem is mathematically advanced thus is omitted here but somewhat trivial because it simply implies that we can assign each point in space an electric potential for any arbitrary charge distribution.} and states that there exists an unique solution to the equation

\begin{equation}
	-\laplacian \Phi = - \div{(\grad{\Phi} )} = \div{\vb{T} } \implies \div{(\vb{T} + \grad{\Phi} )} = 0.
\end{equation}

But since the divergence of a curl is always zero, we have

\begin{equation}
	\vb{T} = - \grad{\Phi } + \curl{\vb{A} }
\end{equation}

Testing the divergence of \(\vb{T} \), we have\footnote{Here and after, we will omit the limit of the integrals.}

\begin{equation}
	\begin{aligned}
		\div{\vb{T} } &= -\div{(\grad{\Phi} )} + \div{(\curl{\vb{A} } )} = -\laplacian \Phi \\ 
		&= -\frac{1}{4\pi } \int \boldsymbol{\nabla}' \cdot \vb{T} (\vb{r} ') \laplacian (\frac{1}{\rcurs } ) d\tau ' \\ 
		&= \int \boldsymbol{\nabla}' \cdot \vb{T} (\vb{r} ') \delta ^3(\vb{r} -\vb{r} ')d\tau ' = \boldsymbol{\nabla} \cdot \vb{T} (\vb{r}). \label{divT}  
	\end{aligned}
\end{equation}

where \(\boldsymbol{\nabla}' \cdot \vb{T} (\vb{r} ') d\tau '\) is taken out of the Laplacian since it depends only on \(\vb{r} '\) but not \(\vb{r} \). 

Testing the curl,

\begin{equation}
	 \curl{\vb{T}} = -\curl{\grad{\Phi}} + \curl{(\curl{\vb{A}})} = -\laplacian{\vb{A}} + \grad{(\div{\vb{A}})}. \label{int1} 
\end{equation}

Now, the first term is simply \(\curl{\vb{T} }\) in a similar fashion as the divergence, and the second term is zero, since 

\begin{equation}
	\begin{aligned}
		4\pi (\div{\vb{A} } ) &= \int \div{\left(\frac{\boldsymbol{\nabla}' \cross \vb{T} (\vb{r} ')}{\rcurs }\right)} d\tau ' \\ 
		&= \int (\boldsymbol{\nabla}' \cross \vb{T} (\vb{r} ')) \cdot \grad{\left(\frac{1}{\rcurs } \right)} d\tau ' = -\int (\boldsymbol{\nabla}' \cross \vb{T} (\vb{r} ')) \cdot \boldsymbol{\nabla}'\left(\frac{1}{\rcurs } \right) d\tau ' \\
		&= \int \frac{1}{\rcurs }\boldsymbol{\nabla}' \cdot (\boldsymbol{\nabla}' \cross \vb{T} (\vb{r} ')) d\tau  - \oint \frac{1}{\rcurs } (\boldsymbol{\nabla}' \cross \vb{T} (\vb{r} ')) \cdot d\vb{A}  \\
        &= - \int \frac{1}{\rcurs } \div{(\curl{\vb{T} (r')} )} d\tau = 0.  	
	\end{aligned}
\end{equation}

where we used \( \grad{\left(1 /\rcurs \right)} = - \boldsymbol{\nabla}'\left(1/\rcurs  \right) \) since the derivative of \(\rcurs = \left| \vb{r} - \vb{r} ' \right| \) with respect to the primed coordinates differ by a sign from that with respect to unprimed coordinates.\footnote{More precisely, \(\partial f(x-x')/\partial x  = - \partial f(x-x')/\partial x' \).} Also, we assumed that \(\curl{\vb{T} } \) goes to zero faster than \(1 /r^2 \) as \(r \to  \infty\) so the surface integral goes to zero.    

Of course we still have to prove that the given forms for the divergence and curl of \(\vb{T} \) converge. At the large \(r'\) limit, they have the form

\begin{equation}
	\int \frac{X(r')}{r'}r'^2 dr', 
\end{equation}

where \(X(r')\) stands for \(\div{\vb{T} } \text { or } \curl{\vb{T} }\).   

Therefore, we also require that the divergence and curl of \(\vb{T} \) goes to zero more repaidly than \(1 /r^2 \) as \(r \to  \infty\) for the proof to hold. 

The solution is unique in a sense that no function that has zero divergence and zero curl everywhere goes to zero at infinity. Therefore no constant function can be added to \(\vb{T} (\vb{r} )\) which doesn't change the divergence and curl of \(\vb{T} \). 

As a matter of fact, any differentiable vector function regardless of its behavior at infinity can be written as a gradient and a curl, as proved above, just that there solution is not given by the Helmholtz theorem.

\end{proof}




\chapter{Integration}
\section{Multiple Integration}

Multiple integration includes single, double and triple integrations, and they sum up the scalar field times an infinitesimal quantity, namely \(f(x)dx, f(x,y)dA = f(x,y)dxdy\) and \(f(x,y,z)dxdydz\).\footnote{ This pattern can obviously be generalized to higher dimensions, but in physics we are only concerned with the three cases.}\footnote{Depending on whether \(dA = dxdy \text { and } dV = dxdydz\) are written explicitly, two or three integral signs will be used.}\footnote{Here the argument \((x,y,z)\) can be unambigously interchanged by \((\vb{r} )\), where \(\vb{r} = x \vu{i} + y \vu{j} + z \vu{k} \) is the position vector. The relation between the two forms can be given explicitly as \(x = r_{x}, y = r_{y}, z = r_{z}\). The only difference is that the latter notation emphasize that the (scalar or vector field) depends only on the location in space and does not specify the usage of Cartesian coordinates and is thus more abstract but general.} 

Refering to \cref{doubleintegral}, we can see that a double integral can be evaluated in two different ways. 

\onefig{doubleintegral}{scale=0.3} 

The first is to sum up all the horizontal strips, then 

\begin{equation}
    I = \int_{c}^{d} \left( \int_{x_1 (y)}^{x_2 (y)} f(x,y) dx \right) dy,   
\end{equation}

where \(x_1 (y) \text { and } x_2 (y)\) are the equations of the curves \(TSV \text { and } TUV\) respectively, and the parenthesis is usually omitted.

The second way is to sum up all the vertical strips, then

\begin{equation}
    I = \int_{a}^{b} \int_{y_1 (x)}^{y_2 (x)} f(x,y) dy dx,
\end{equation}

where \(y_1 (x) \text { and } y_2 (x)\) are the equations of the curves \(STU \text { and } SVU\) respectively. Thus the order of integration does not matter (given that \(f(x,y)\) behaves properly) as long as the limits of integrals are correct.

Double integration can be regarded as calculating a volume which the base is flat, and the height is \(f(x,y)\), as the same way as how a single integration is almost always used to find the area, where the side is straight, and the height being \(f(x)\). 

If the integrand \(f(x,y)\) can be written as a product of functions of \(x \text { and } y\) separately, \textit{i.e.,} \(f(x,y) = f(x)f(y)\), then we can evaluate each integral independently as \(\left( \int f(x)dx \right)\left( \int f(y)dy \right)\).\footnote{The same applies to summation, as integration and summation are essentially the same process.} 

The above analysis can be extended to triple integration easily.

\example{Volume of a Tetrahedron.}
{Find the volume of the tetrahedron bounded by the three coordinate surfaces \(x=0, y=0 \text { and }  z=0\) and the plane \(x/a + y /b + z /c =1\) as shown in \cref{tetrahedron}.}
{The volume can be written as a double or triple integral, depending on how one inteprets the summing process. In triple integral,

\begin{equation}
	V = \int_{0}^{a} \int_{0}^{b-\frac{bx}{a} } \int_{0}^{c\left(1-\frac{y}{b} - \frac{x}{a}  \right)} dxdydz,
\end{equation}

which can be easily reduced to a double integral by evaluating the integral involving \(z\) as

\begin{equation}
	V = \int_{0}^{a} \int_{0}^{b-\frac{bx}{a}}  c \left( a-\frac{y}{b} - \frac{x}{a}\right) dxdy
\end{equation}

The result is \(V = abc /6 \). } 
\onefig{tetrahedron}{scale=0.3}

\section{Line, Surface and Volume Integration}

Line, surface and volume sum up scalar or vector field times an infinitesimal quantity. The infinitisimal quantities are (in Cartesian coordinates), \(d\vb{r} = dx \vu{i} + dy \vu{j} + dz \vu{k}, d\vb{S} \footnote{The general form of \(d\vb{S} \) is not trivial. It is exactly this property which makes surface integral the hardest type to deal with.}\text { and } dV = dxdydz\),\footnote{\(dV\) is a scalar since an infinitesimal volume has no preferred direction in three-dimensional space.} out of which the first two are vectors and the last one is scalar.\footnote{Note that if the final form of integrands contain vectors then we must convert it into Cartesian coordinates, since it is the only coordiante system where the unit vectors are fixed (and so can be brought out of the integral sign).} 

\subsection{Curves and Surfaces}

In evaluating line and surface integrals, we always have to specify what curve and surface are we integrating over, and we therefore need to devise a systematic way to describe these curves and surfaces. For both curves and surfaces, they can be represented in parameterized form, where an extra variable(s) is(are) introduced to specify a point on the curve or surface. Alternatively, they can be represented algebrically.

\subsubsection{Curves}

A curve in space\footnote{In the following discussion we assume that the surfaces are orientable, \textit{i.e.,} there is a consistent choice for the normal vector \(\vu{n} \) which varies smoothly over the surface. Exmples of non-orientable surfaces include the Möbius strip and the klein bottle.} can be described by the vector \(\vb{r} (u)\) joining the origin \(O\) of a coordinate system to a point on the curve. It can be viewed as a map\(f: \mathbb{R} \to \mathbb{R}^{3} \), where we assign each parameter \(u\) in \(\mathbb{R}\) a corresponding vector in \(\mathbb{R}^{3} \).  As the parameter \(u\) varies, the end-point on the curve moves along the curve.\footnote{A useful visualization is imagine sliding the sidebar for \(u\) on Desmos and the point corresponding point will move along the curve accordingly.} Some common examples of parameterized curves include

\begin{equation}
	\vb{r} (t) = (\cos t, \sin t, t), 
\end{equation}

which describes a helical path of a particle if \(t\) is time. We also note that the choice of parameterization is not unique. The parameterized curve \(\vb{r} (t) = (\cos \lambda t, \sin \lambda t, \lambda t)\) describes the same helix for all \(\lambda \neq 0\).\footnote{Sometimes, one may ponder upon an unwise choice of parameterization in which the derivative \(\dot{\vb{r} } \) vanishes at some point. For example, consider the curve \(\vb{r} (t) = (t^3 ,t^3 )\), which is just the straight line \(x=y\). However, \(\dot{\vb{r} } \) vanishes at \(t=0\). Using the parameterization \(\vb{r} (t) = (t,t)\) which describes the same curve, we find that this vanishing is not a property of the curve but of our choice of parameterization. We will assume that the curves we encounter are always regularly parameterized, which ensures proper definition of relevant parameters such as the tangent vector and the arc length.}  

Another example is the unit circle, which can be parameterized by 

\begin{equation}
	\vb{r} (t) = (\cos t, \sin t) ~\text { or }~ \left( \frac{2t}{t^2+1}, \frac{t^2-1}{t^2+1}   \right).\footnote{With the exception of the point \((0,1)\) since we have \((t^2-1)/(t^2+1) < 0\) for all \(t\), so the y-coordinate would not be equal to 1 unless \(t = \infty\), but \(t\) in reality only tends to infinity.}
\end{equation}

Often \(x = t\) so we can simply treat \(x\) as the parameter and express the integral in terms of \(x\). 

But the most common parameterization is a straight line, given a point \(\vb{r} _{0} =  (x_0 ,y_0 ,z_0 )\) on the line and a vector \(\vb{d} = (a,b,c)\) along the line, which has the form

\begin{equation}
	\vb{r} (t) = \vb{r} _{0}+ t\vb{d} = (x_0 + at, y_0 + bt, z_0 +ct),
\end{equation}

then the infinitesimal displacement vector is simply given by 

\begin{equation}
	d\vb{r} (t) = \vb{d} dt = (a,b,c)dt.
\end{equation}



Alternatively, a space curve in three-dimensional space can be represented by two simultaneous equations algebrically as \(F(x,y,z) = G(x,y,z) = 0\). For example, the first example above can be described equivalently as \(x - \cos z = y - \sin z = 0\). However, we lost the picture of the particle's position evloving through space, and it is not a convenient form to work in integrals. 

\subsubsection{Surfaces}

A point on a surface, on the other hand, requires two parmeters \(u \text { and } v\), and it can be viewe as a map \(f:\mathbb{R}^2 \to \mathbb{R}^3 \). A common example of parameterized curves is 

\begin{equation}
	\vb{r} (\theta ,\phi ) = R(\sin \theta \cos \phi , \sin \theta \sin \phi , \cos \theta ),
\end{equation}

which describes the surface of a sphere with radius \(R\).

Alternatively, a space surface in three-dimensional space can be represented by a single equation algebrically as \(F(x,y,z) = 0\), or \(z = f(x,y)\).  

There are generally two ways to compute an infinitesimal surface \(dS\). If the parametric form of the surface is given, then refer to \cref{jacobian}, since \(v\) is constant along \(KL\), the line element \(KL\) can be written as 

\begin{equation}
	d\vb{r} _{KL} = \frac{\partial \vb{r} }{\partial u}du+ \frac{\partial \vb{r} }{\partial v}dv = \frac{\partial \vb{r} }{\partial u}du = \frac{\partial x}{\partial u} du \vu{i} + \frac{\partial y}{\partial u} du \vu{j}.
\end{equation}

Similarly, \(d\vb{r} _{KN}  = (\partial x / \partial v) dv \vu{i} + (\partial y/\partial v)dv \vu{j}\), thus the area of the parallelogram \(KLMN\) is given by

\begin{equation}
	dS_{u,v} = \abs{d\vb{r} _{KL} \cross  d\vb{r} _{KN}} = \abs{\frac{\partial \vb{r} }{\partial u} \cross \frac{\partial \vb{r} }{\partial v} }dudv = \abs{\frac{\partial x}{\partial u} \frac{\partial y}{\partial v}  - \frac{\partial x}{\partial v}  \frac{\partial y}{\partial u} }dudv = \abs{\frac{\partial (x,y)}{\partial (u,v)} } dudv.
\end{equation}

Since both \(\partial \vb{r} /\partial u \text { and } \partial \vb{r} /\partial v  \) lie in the tangent plane to the surface at \(\vb{r} \), the direction of the infinitesimal surface is also encoded in the cross product

\begin{equation}
	d\vb{S} _{u,v} =\left( \frac{\partial \vb{r} }{\partial u} \cross \frac{\partial \vb{r} }{\partial v} \right) dudv.   
\end{equation}


\onefig{jacobian}{scale=0.4} 

If the algebraic form of the surface is given, then we usually project the surface \(S\) onto the \(xy\)-plane (sometimes the \(y\)-\(z\) or \(x\)-\(z\) planes are more useful). From \cref{dS}, we see that 

\begin{equation} \label{dseq} 
	dA = \abs{\cos \alpha }dS \implies dS = \frac{dA}{\vu{n} \cdot \vb{k} } = \frac{dA}{\frac{\grad{f} }{\abs{\grad{f} } } \cdot \vu{k}  } =  \frac{\abs{\grad{f} } }{\partial f/\partial z}dA \implies d\vb{S} = \frac{\grad{f}}{\partial f/\partial z } dA, 
\end{equation}

where \(\alpha \) is the angle between the unit vector \(\vu{k} \) in the \(z\)-direction and the unit normal \(\vu{n} \) to the surface, and \(f(x,y,z)=0\) is the equation which describe the surface.

\onefig{dS}{scale=0.4} 

Using the above equation, we can convert any surface integral over \(S\) as a double integral over the region \(R\) in the \(x\)-\(y\) plane.

Note that in the above discussion, however, tht we assumed any line parallel to the \(z\)-axis only intersects \(S\) once. If this is not the case, we must split up the surface into smaller surfaces \(S_1, S_2 \textit{ etc.}\) Also, sometimes instead of projecting the surface onto the \(xy\)-plane, it might be easier to project it onto the \(z\)-\(x\) or the \(y\)-\(z\) plane.     

A surface that comes up all the time is a spherical surface with raidus \(R\). Parametrically, the surface is described as 

\begin{equation}
	\vb{r} (\theta ,\phi ) = R \vu{r} = R(\sin \theta \cos \phi , \sin \theta \sin \phi , \cos \theta ),
\end{equation}

so we have  

\begin{equation}
	\begin{aligned} 
	d\vb{S} &= \left( \frac{\partial \vb{r} }{\partial \theta }  \cross \frac{\partial \vb{r} }{\partial \phi }   \right) d \theta d\phi = R^2 \begin{pmatrix}
		 \cos \theta \cos \phi  \\
		 \cos \theta \sin \phi  \\
		 -\sin \theta  \\
	\end{pmatrix} \cross \begin{pmatrix}
		 -\sin \theta \sin \phi  \\
		 \sin \theta \cos \phi  \\
		 0 \\
	\end{pmatrix} d \theta d \phi \\
	&= R^2 \sin \theta d \theta d\phi  (\sin \theta \cos \phi , \sin \theta \sin \phi , \cos \theta ) = R^2\sin \theta d \theta d\phi \vu{r} .
	\end{aligned} 
\end{equation}

Algebrically, the surface is described as 

\begin{equation}
	f(x,y,z) = x^2+y^2+z^2 - R^2 = 0,
\end{equation}

so we have

\begin{equation}
	\begin{aligned} 
	d\vb{S} &= \frac{\partial z}{\partial f} dA \grad{f} = \frac{1}{2z} dxdy 2(x,y,z) = \abs{\frac{\partial (x,y)}{\partial (\theta ,\phi )} }d \theta d\phi \left(\frac{x}{z},\frac{y}{z},1  \right) \\
	&= (R^2\cos \theta \sin \phi )(\tan \theta \cos \phi , \tan \theta \sin \phi ,1) d \theta d \phi  = r^2\sin \theta d \theta d\phi \vu{r} .
	\end{aligned} 
\end{equation}


\example{Space Surface.}
{Consider the surface \(\phi (x,y,z) = c\), 

\begin{enumerate}
	\item Show that \(\grad{\phi } \) is a vector perpendicular to the surface \(\phi \).
	\item Let \(R\) be the distance from a fixed point \(A\) to any point \(P\) on the surface \(\phi \). Show that \(\grad{R} \) is a unit vector in the direction \(AP\). 
	\item Let \(P\) be any point on an ellipse whose foci are at points \(A \text { and } B\). Given that the distances \(AP \text { and } BP\) obeys \(AP + BP = s\), prove that lines \(AP \text { and } BP\) make equal angles with the tangent \(T\) to the ellipse at \(P\).       
	\item Imagine a source is emitting sound waves at focus \(A\) of the ellipse. Give a physical interpretation of the result in \((c)\). 
\end{enumerate}
~
}
{\begin{enumerate}
	\item From 
	
	\begin{equation}
		d \phi = \grad{\phi } \cdot d\vb{r}
	\end{equation}
	
	where \(\vb{r} \) is the position vector of an arbitrary point on the surface and \(d\vb{r} \) is an arbitrary tangent vector that lies on the surface, we have \(\grad{\phi } \perp \text{ every tangent vectors} \). 
	\item We calculate the gradient of \(R= ((x-x_0 )^2+(y-y_0 )^2+(z-z_0 )^2)^{1 /2} \) to be
	\begin{equation}
		(\grad{R} )_{i} = \partial _{i} \left( (x_{i} - c_{i}  ) (x_{i } - c_{i}  )\right)^{1 /2} = \left( (x_{i}-c_{i}  )(x_{i} - c_{i}  ) \right)^{-1/2} (x_{i} - c_{i}  ) = \frac{\vb{R}_{i} }{\abs{\vb{R} } }.      
	\end{equation}
	\item Using the fact that \(AP + BP = s\), the ellipse can be described by the equation \(\phi (x,y) = AP + BP = s\). From \((a) \text { and } (b)\), we have the normal vector \(\vb{n} \)
	\item  
	\begin{equation}
		\vb{n} = \grad{\phi (x,y)} = \grad{AP} + \grad{BP} = \frac{\vb{A} \vb{P} }{AP} + \frac{\vb{B} \vb{P} }{BP}     
	\end{equation}

	The equal angles can be shown by 

	\begin{equation}
		\frac{\vb{A} \vb{P} }{AP} \cdot \vb{n} = 1 + \frac{\vb{A} \vb{P} \cdot \vb{B} \vb{P} }{(AP)(BP)} = \frac{\vb{B} \vb{P} }{BP} \cdot \vb{n} = \frac{\vb{B} \vb{P} \cdot \vb{A} \vb{P} }{(BP)(AP)} + 1.    
	\end{equation}
	
	Alternatively, we can desribe the ellipse using 

	\begin{equation}
		\phi (x,y) = \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1 \implies \grad{\phi (x,y)} = \left( \frac{2x}{a^2},\frac{2y}{b^2}   \right).
	\end{equation}

	Explicitly, we have

	\begin{equation}
		AP = \left( (x+c)^2+y^2 \right)^{1 /2} ~\text { and }~ BP = \left( (x-c)^2+y^2 \right) ^{1 /2}, 
	\end{equation}

	with their gradients

	\begin{equation}
		\grad{AP} = \frac{(x+c)\vu{i} +y \vu{j} }{AP} ~\text { and }~ \grad{BP} = \frac{(x-c)\vu{i} +y \vu{j} }{BP}.    
	\end{equation}

	The equal angles can be shown by 

	\begin{equation}
		\grad{AP} \cdot \grad{\phi } = \grad{BP} \cdot \grad{\phi }.    
	\end{equation}
	
	Substituting and expanding we get 

	\begin{equation}
		a^2y^2 + a^2x^2 + a^2c^2- a^{4} - x^2c^2 = 0, 
	\end{equation}
	
	which is indeed true once we substitute \(c^2 = a^2-b^2\) and the ellipse equation \(x^2 /a^2 + y^2 /b^2 = 1\).  
	
	\item If a point source at \(A\) emits sound, each ray travles to the ellipse and reflects according to the law of reflection. By \((c)\), every sound-ray leaving \(A\) and hitting the inner boundary of the ellipse will be reflected straight to the other focus \(B\). Thus a listener at \(B\) hears sound from \(A\) clearly no matter which idrection it intially left \(A\).
	
\end{enumerate}
~
} 




\subsubsection{Frenet-Serret Equations}

We can also define an orthogonal coordinate system that is intrinsic to the curve. For the three axes, an obvious choice would be the tangential direction, whose unit vector can be defined by

\begin{equation}
	\vu{t} = \frac{d\vb{r} }{ds} = \frac{d \vb{r} }{\abs{d \vb{r} } }.
\end{equation}

The curvature \(\kappa \) and the pricinpal normal unit vector are defined together from the rate of change of \(\vu{t} \) with respect to \(s\)

\begin{equation}
	\kappa \vu{n}  = \frac{d \vu{t} }{ds} = \frac{d^2\vb{r} }{ds^2}
\end{equation}

and the radius of curvature is defined as \(\rho = 1/\kappa  \). We see that the radius of curvature measures the second derivative, capturing how much the curve fails to be a straight line. Both \(\vu{n} \text { and } \vu{t} \) lie on the plane that the small segements around \(\vb{r} \) form.  

The binormal unit vector is then defined as the unit vector perpendicular to both \(\vu{t}  \text { and } \vu{n} \) 

\begin{equation}
	\vu{b} = \vu{t} \cross \vu{n} .
\end{equation}

We can similarly define the torsion \(\tau \) as the rate of change of \(\vu{b} \) with respect to \(s\)  

\begin{equation}
	-\tau \vu{n} =  \frac{d \vu{b} }{ds}, 
\end{equation}

and radius of torsion as \(\sigma  = 1/ \tau  \). We see that the radius of torsion measures the third derivative, capturing how much the curve fails to be planar.

In summary, \(\vu{t} , \vu{n} \text { and } \vu{b} \) and their derivatives with respect to \(s\) are related to one another by the relations (called the Frenet-Serret formulae) 

\begin{equation}
	\frac{d \vu{t} }{ds} = \kappa \vu{n} , ~~\frac{d \vu{n} }{ds} = \tau \vu{b} - \kappa \vu{t}~~ \text { and } ~~\frac{d \vu{b} }{ds} = -\tau \vu{n} .  
\end{equation}

Note that the shape of a curve is completely determined by \(\kappa \text { and } \tau \), for example, a curve is planar if \(\tau = 0\), and a curve in which \(\vu{t} \) makes a constant angle with a fixed direction, \textit{i.e.,} when either \(\kappa \text { or } \tau \) is independent of \(s\). 



\example{Acceleration of a Particle.}
{Show that the acceleration of a particle travelling along a trajectory \(\vb{r} (t)\) is given by 

\begin{equation}
	\vb{a} (t) = \frac{dv}{dt} \vu{t} + \frac{v^2}{\rho } \vu{n} .  
\end{equation}
~
}
{The velocity of the particle is \(\vb{v} (t) = v \vu{t} \), so the acceleration is 

\begin{equation}
	\vb{a} (t) = \frac{d\vb{v} }{dt} = \frac{dv}{dt} \vu{t} + v \frac{d \vu{t} }{dt} = \frac{dv}{dt}\vu{t} + v \frac{ds}{dt}\frac{d \vu{t} }{ds} = \frac{dv}{dt} \vu{t} + \frac{v^2}{\rho }\vu{n} .       
\end{equation}
~
}

\subsection{Line Integrals}

Integrals along a line can involve vector and scalar fields. There are four kinds of line integrals,\footnote{However usually only the first and the third integrals are used often enough to be recognized as line integrals.}  namely

\begin{equation}
	\int _{C} f(\vb{r} ) ds, ~~ \int_{C}^{} f(\vb{r} ) d\vb{r} , ~~ \int_{C}^{} \vb{F} (\vb{r} ) \cdot d\vb{r} ~ \text { and }  \int_{C}^{} \vb{F} (\vb{r} ) \cross d\vb{r} . 
\end{equation}

The curve \(C\) can be open, \textit{i.e.,} the beginning and end point are not the same; or closed, where \(C\) is a closed loop and we will add a circle to the integral sign as \(\oint_{C} \). For a closed curve the direction of integration is conventionally taken to be anticlockwise.

Note that for the first type of the four line integrals above \(ds = \abs{d \vb{r} } \) is an intrinsically positive quantity, which represent an infinitesimal arc length in \(\mathbb{R}^3 \). On the other hand, \(dx\) in a single integral represents a straight infinitesimal interval element in \(\mathbb{R}\), so it carries sign. The integral is typically carried out as

\begin{equation}
	\begin{aligned} 
	s &= \int_{\vb{a} }^{\vb{b} }f(\vb{r} )ds = \int_{\vb{a} }^{\vb{b} } f(\vb{r} )\abs{d \vb{r} (u)} = \int_{u_{\vb{a} } }^{u_{\vb{b} }  } f(\vb{r} )\abs{\frac{d\vb{r} }{du} } \abs{du}  \\
	&= \int_{u_{ \vb{a}} }^{u_{ \vb{b}} }f(\vb{r} ) \sqrt{\left( \frac{dx}{du}\right)^2 + \left( \frac{dy}{du}  \right)^2 + \left( \frac{dz}{du}  \right)^2} \abs{du},\\
	\end{aligned} 
\end{equation}

If \(u_{\vb{b} } > u_{\vb{a} }  \), then \(du\) is positive and we can omit the absolute sign of \(du\). If \(u_{\vb{b} } < u_{\vb{a} }  \), then \(du\) is negative, and we can omit the absolute sign of \(du\) if we add a negative sign in front. This is equivalent of reversing the limit of the integrations \((u_{\vb{a} } \text { and } u_{\vb{b} }  )\). Therefore, the limit of integration does not matter when evaluating the first type of the four line integrals above. 

\example{Line Integral (1).}
{Consider the scalar function \(\phi (\vb{r} ) = 2\). Evaluate the line integral \(I = \int_{C}^{} \phi ds \), along a straight line from \((0,1)\) to \((1,0)\) and in the reverse direction.}
{The forward and backward parameterizations are

\begin{equation}
	\vb{r}_{1}  (u) = (u,0) ~\text { and }~ \vb{r}_{2} (u) = (1-u,0).
\end{equation}

The forward line integral is 

\begin{equation}
	I = \int_{C}^{} \phi ds = \int_{0}^{1} 2 \abs{\frac{d\vb{r} _{1} }{du} } du = 2 \int_{0}^{1} \sqrt{1^2 + 0^2} du = 2.     
\end{equation}

The backward line integral is 

\begin{equation}
	I = \int_{C}^{} \phi ds = \int_{0}^{1} 2 \abs{\frac{d\vb{r} _{2} }{du} } du = 2 \int_{0}^{1} \sqrt{1^2 + 0^2} du = 2.     
\end{equation}

The limits of the line integral in both cases are 0 to 1, due to the different paratermization used. Of course, we can use the same parameterization in the second line integral as the first one, where the limits of integration will be from 1 to 0, but then we should not replace \(\abs{du} \) by \(du\), but should add a negative sign, which gives the same answer.  
} 


\example{Line Integral (2).}
{Evaluate the line integral \(I = \int_{C}^{} \vb{F} \cdot d\vb{r}  \) from \((1,1)\) to \((4,2)\), where \(\vb{F}  = (x+y)\vu{i} + (y-x)\vu{j} \), along 
\begin{enumerate}[itemsep=10pt]
	\item the parabola \(y^2 = x\),
	\item the curve \(x = 2u^2+u+1, y= 1+u^2\), and
	\item the line \(y=1\) from \((1,1)\) to \((4,1)\), followed by the line \(x = 4\) from \((4,1)\) to \((4,2)\).      
\end{enumerate}~
}
{Evaluating the dot product explicitly, we have 

\begin{equation}
	 I = \int_{(1,1)}^{(4,2)} (x+y)dx + (y-x)dy.
\end{equation}

\begin{enumerate}
	\item Along the parabola \(y^2 = x\), we have \(2ydy = dx\), so 
	
	\begin{equation}
		I = \int_{1}^{2} ((y^2+y)2y + (y - y^2))dy = \frac{34}{3}.    
	\end{equation}
	
	\item We have \(dx = (4u+1)du \text { and } dy = 2udu\), so
	
	\begin{equation}
		I = \int_{0}^{1} ((3u^2+u+2)(4u+1)-(u^2+u)(2u))du = \frac{32}{3}.  
	\end{equation}
	
	\item We split the integral into two parts, then 
	
	\begin{equation}
	\begin{aligned} 
		I &= \int_{(1,1)}^{(4,1)} ((x+y)dx + (y-x)dy) + \int_{(4,1)}^{(4,2)} ((x+y)dx + (y-x)dy) \\ &= \int_{1}^{4} (x+1)dx + \int_{1}^{2} (y-4)dy = 8.      
	\end{aligned} 
    \end{equation}
\end{enumerate}~
} 


\example{Line Integrals (3).}
{Evaluate the line integral \(I = \oint_{C} x dy\), where \(C\) is the circle in the \(xy\)-plane defined by \(x^2 + y^2 = a^2, z=0\). }
{Since \(x\) is not a single-valued function of \(y\), we must divide the path into two parts with \(x = +\sqrt{a^2-y^2} \) for \(x\geq 0\) and \(x = -\sqrt{a^2-y^2} \) for \(x \le 0\). So

\begin{equation}
	I = \int_{-a}^{a} \sqrt{a^2-y^2}dy + \int_{a}^{-a} \left(-\sqrt{a^2-y^2}\right)dy= \pi a^2    .
\end{equation}

Alternatively, we can represent the entire circle parametrically, by \(x = a \cos \phi , y = a \sin \phi \) with \(\phi \) running from \(0\) to \(2 \pi \) and we have 

\begin{equation}
	I = a^2 \int_{0}^{2\pi } \cos ^2 \phi d\phi  = \pi a^2.
\end{equation}
} 

\example{Line Integral (4).}
{Evaluate the line integral \( I = \int_{C}^{} (x-y)^2 ds \), where \(C\) is the semicircle of radius \(a\) running from \(A = (a,0)\) to \(B = (-a,0)\) and for which \(y \ge 0\).}
{Introducing a parametric variable \(\phi \) running from \(0\) to \(2\pi \), we have 

\begin{equation}
	\vb{r} (\phi ) = a \cos \phi \vu{i} + a \sin \phi \vu{j} ~\text { and }~  ds = \frac{d\vb{r} }{d\phi }d\phi   = a d\phi. 
\end{equation}

Thus

\begin{equation}
	I = \int_{0}^{\pi } a^3 (1-\sin 2\phi ) d\phi = \pi a^3 . 
\end{equation}
} 

\example{Line Integral (5).}
{Evaluate the integral \(\oint_{C} (dx + xydz)\), where \(C\) is the closed curve forming the boundary of the surface given by the equation \(z = x^2 - y^2\), for \(x^2 + y^2 \le 1\).}
{Parametrizing the curve in terms of \(\theta \), the integral becomes 

\begin{equation}
	\oint_{C} (dx+xydz) = \int_{0}^{2\pi }(-\sin \theta + \cos \theta \sin \theta  (-2\cos \theta \sin \theta -2\sin \theta \cos \theta ))d \theta = -\pi .  
\end{equation}

Alternatively, we can use Stokes' theorem to rewrite the integral as 

\begin{equation}
	\oint_{C} (dx+xydz) = \int_{S}^{} \curl{\vb{F} (\vb{r} )}\cdot d\vb{S} ,    
\end{equation}

where \(\vb{F} (\vb{r} ) = (1,0,xy) \implies \curl{\vb{F} (\vb{r} )} = (x,-y,0) \). The infinitesimal surface is 

\begin{equation}
	d\vb{S} = \grad{g(\vb{r} )} = (-2x,2y,1), 
\end{equation}

where \(g(\vb{r} ) = z - f(x,y) = z - x^2 + y^2\). 

Inserting these expressions into the integral we have

\begin{equation}
	\int_{S}^{} \curl{\vb{F} (\vb{r} )}\cdot d\vb{S} = -2 \int_{S'}^{} (x^2 + y^2)dxdy  = -\pi .  
\end{equation}

} 




\section{Surface Integrals}
As with line integrals, integrals over a surface can involve vector and scalar fields. There are four kinds of surface integrals, namely 

\begin{equation}
    \int_{S}^{} \phi(\vb{r}) dS,  ~~ \int_{S}^{} \phi(\vb{r}) d\vb{S} , ~~ \int_{S}^{} \vb{F}(\vb{r}) \cdot d\vb{S} ~ \text { and } \int_{S}^{} \vb{F}(\vb{r}) \cross d\vb{S},    
\end{equation}

where \(d\vb{S} = \vu{n} dS\) is the infinitesimal vector area element. The direction of \(\vu{n} \) is conventionally assumed to be directed outwards from the closed volume if the surface is closed; or given by the right-hand rule if the surface is open and spans some perimeter curve \(C\). 

\(dA\) is used in double integral to represent a flat infinitesimal area element \(\in \mathbb{R}^2\). \(dS\) is used in surface integral to represent an arbitrary infinitesimal surface element \(\in \mathbb{R}^3 \). So the main difference between the first integral and an ordinary double integral is that the surface \(S\) here can span through three dimensions while the area \(A\) in double integral is confined to two dimensions.

Entirely analogous to the fact that the first type of line integrals in the previous subsection is independent of the limits of integration (\textit{i.e.,} the orientation of the curve), the firsty type of the four surface integrals above is also independent of the orientation of the surface (\textit{i.e.,} the limits of integration). 

\example{Surface Integral(1).}
{Find the element of area on the surface of a sphere of radius \(a\), and hence calculate the total surface area of the sphere.}
{We can represent a point \(\vb{r} \)  on the surface of the sphere in terms of the two parameters \(\theta \text { and } \phi \) as

\begin{equation}
	\vb{r} = a \sin \theta \cos \phi \vu{i} + a \sin \theta \sin \phi \vu{j} + a \cos \theta \vu{k} .
\end{equation}

An infinitesimal area is given by 

\begin{equation}
	dS = \abs{\frac{\partial \vb{r} }{\partial \theta } \cross \frac{\partial \vb{r} }{\partial \phi } } = a^2\sin \theta d \theta d \phi 
\end{equation}

And the total surface area is then 

\begin{equation}
	S = \int_{0}^{\pi }\int_{0}^{2\pi } a^2 \sin \theta d \theta d \phi = 4\pi a^2.    
\end{equation}
} 

\example{Surface Integral (2).}
{Evaluate the surface integral \( I = \int_{S}^{} \vb{F}(\vb{r} ) \cdot d\vb{S} \), where \(\vb{F}  = x \vu{i}  \) and \(S\) is the surface of the hemisphere \(x^2 + y^2 + z^2 = a^2\).}
{Parameterizing the surface by \(\theta \text { and } \phi \), 

\begin{equation}
	\vb{F} \cdot d\vb{S}  = x(\vu{i} \cdot \vu{r} )dS = (a \sin \theta \cos \phi )(\sin \theta \cos \phi )(a^2\sin \theta d \theta d\phi ),
\end{equation}

where we have used

\begin{equation}
	\vu{i} \cdot \vu{r} = \vu{i} \cdot \frac{\vb{r} }{\abs{\vb{r} } } = \vu{i} \cdot \frac{x \vu{i} + y \vu{j} + z \vu{k} }{a} = \frac{x}{a}.
\end{equation}

So the integral becomes

\begin{equation}
	I = a^3 \int_{0}^{\frac{\pi }{2} } \sin ^3 \theta d \theta \int_{0}^{2\pi } \cos ^2\phi d\phi = \frac{2\pi a^3 }{3}.    
\end{equation}

Alternatively, we can describe the surface of the hemisphere as \(f(\vb{r}) = x^2 + y^2 + z^2 - a^2 = 0\), so we have 

\begin{equation}
	\abs{\grad{f} } = 2\abs{\vb{r} } = 2a, ~~ 	\frac{\partial f}{\partial z} = 2z = 2\sqrt{a^2-x^2-y^2}.
\end{equation}

Therfore the integral becomes

\begin{equation}
	I = \int \int_{R}^{} \frac{x^2}{\sqrt{a^2-x^2-y^2} }dxdy = \frac{2\pi a^3 }{3},
\end{equation}

where the integral is evaluated by changing to polar coordinates.
 } 
 
\example{Surface Integral (3).}
{Evaluate \(\int_{S}^{}   \vb{F} (\vb{r} )\cdot d\vb{S} \) over the surface bounded by the paraboloid \(y = x^2+z^2\) for \(y \in [0,1]\), and the disc \(x^2+z^2 \le 1\) at \(y=1\), for \(\vb{F} = (0,y,-z)\).}
{The algebraic form of the curved surface is \(g(\vb{r}) = y - f(x,z) = y - x^2 - z^2 = 0\), whose gradient is \(\grad{g(\vb{r})} = (-2x,1,-2z) \) (and \(\partial g /\partial y= 1\)). This vector points inward to the surface, so we should change its sign to adhere to the normal convention of outward-pointing normals. The integral over the curved surface then becomes

\begin{equation}
	\int_{S'}^{}   \vb{F(\vb{r} )} \cdot \grad{g(\vb{r})} dxdz = \int_{S'}^{} (-x^2-3z^2)dxdz,
\end{equation}

where the surface \(S'\) is the projection of the paraboloid onto the \(xz\)-plane, which is a circle, which is most conveniently parameterized by the polar coordinates \((r,\theta )\), so the integral becomes

\begin{equation}
	\int_{S'}^{} (-x^2-3z^2)dxdz = \int_{0}^{2\pi } d \theta \int_{0}^{1} r(-r^2\cos ^2\theta - 3r^2\sin ^2\theta )dr = -\pi .   
\end{equation}

The integral over the end surface is trivial, which upon integration gives \(\pi \), so the two contributions sum to zero. 

} 

\example{Surface Integral (4).}
{Evaluate \(\int_{S}^{} \vb{F} (\vb{r} ) \cdot d\vb{S}  \) over the upper half of the sphere \(x^2+y^2+z^2 = 9\) and for \(\vb{F} (\vb{r} ) = (x,y,z^4)\).}
{The surface is given parametrically by the equation 

\begin{equation}
	\vb{r} (\theta , \phi ) = 3(\sin \theta \cos \phi , \sin \theta \sin \phi , \cos \theta ), \quad \theta \in \left[0,\frac{\pi }{2} \right], \quad \phi \in [0, 2\pi ].
\end{equation}

The infinitesimal surface vector \(d\vb{S} \) can be calculated explicitly by 

\begin{equation}
	d\vb{S} = \frac{\partial \vb{r} }{\partial \theta } \cross \frac{\partial \vb{r} }{\partial \phi } = 9(\sin ^2\theta \cos \phi , \sin ^2\theta \sin \phi , \sin \theta \cos \theta ).   
\end{equation}

Then the integral can be carried out straightforwardly as

\begin{equation}
	\int_{S}^{} \vb{F} (\vb{r} ) \cdot \left( \frac{\partial \vb{r} }{\partial \theta } \cross \frac{\partial \vb{r} }{\partial \phi }   \right) d \theta d\phi = 279\pi .  
\end{equation}

} 

\example{Surface Integral (5).}
{Viviani's window is the part of the surface of the sphere of radius \(R\) centered in the origin, contained within the right cylinder with base given by a circumference of radius \( R/2\), centered at \((x,y) = \left(R /2, 0\right)\), for \(z \ge 0\) (see \cref{vivi}). Find the area \(S\) of the Viviani's window.}
{We are tasked with solving the integral \(S = \int_{S}^{} dS \), and the difficulty lies on parameterizing the surface \(S\). 

Since the surface is inherently a sphere, the algebraic form is \(f(x,y,z) = x^2+y^2+z^2-R^2= 0\). An infinitesimal surface area is then given by 

\begin{equation}
	dS = \frac{\abs{\grad{f(x,y,z)} } }{\partial f/\partial z}dxdy = \frac{R}{\sqrt{R^2-x^2-y^2} }dxdy.  
\end{equation}

Changing to polar coordinates with the substitution \(x = R/2 + \rho \cos \phi \text { and } y = \rho \sin \phi \),

\begin{equation}
	\begin{aligned} 
	S &= \int_{S}^{} dS = \int_{S}^{} \frac{R}{\sqrt{R^2-x^2-y^2} }dxdy \\
	&= \int_{0}^{2\pi } \int_{0}^{R /2} \frac{R}{\sqrt{R^2-(R/2 + \rho \cos \phi )^2 - (\rho \sin \phi )^2} } \rho d\rho d\theta ,   
	\end{aligned}   
\end{equation}

which has no hope to be integrated easily.

Instead, we can parameterize the sphere with the usual \(\theta \text { and } \phi \), where \(\phi \in [-\pi /2, \pi /2]\) and use the cylinder's equation to find the condition of \(\theta \)

\begin{equation}
	\left( x-\frac{R}{2}  \right)^2+y^2 \le \left( \frac{R}{2}  \right)^2 \implies \sin \theta \le \cos \phi \implies \theta \le \sin ^{-1} (\cos \phi ) =  \frac{\pi }{2} - \abs{\phi }.  
\end{equation}

The integral becomes 

\begin{equation}
	S = \int_{S}^{} dS = \int_{-\pi /2}^{\pi /2} \int_{0}^{\pi /2 - \abs{\phi } } R^2\sin \theta d \theta d\phi = R^2(\pi -2).     
\end{equation}

Alternatively, we can parameterize the surface with \(r, \theta \) according to \cref{para}, where 

\begin{equation}
	x = r\cos \theta , \quad y = r\sin \theta ~\text { and }~ z = \sqrt{R^2 - x^2 - y^2} = \sqrt{R^2 - r^2}
\end{equation}

for \( r \in [0,R\cos \theta ], \theta \in \left[-\pi /2, \pi /2  \right]\) 

The infinitesimal surface element is then 

\begin{equation}
	dS = \abs{\frac{\partial \vb{r} }{\partial r} \cross \frac{\partial \vb{r} }{\partial \theta }  } = \frac{rR}{\sqrt{R^2-r^2} }. 
\end{equation}

Thus the integral

\begin{equation}
	S = \int_{-\frac{\pi }{2} }^{\frac{\pi }{2} }d \theta \int_{0}^{R\cos \theta }\frac{rR}{\sqrt{R^2 - r^2} }dr = R^2(\pi -2).     
\end{equation}



} 

\twofig{vivi}{width=\textwidth}{para}{width=\textwidth}{sur4} 

\example{Surface Integral (6).}
{Find the surface area of a cone defined by the equation \(4x^2+y^2=9z^2\), and \(0 \le z \le 1\).  }
{The cone surface can be parameterized by the \(\rho \text { and } \theta \) as

\begin{equation}
	\vb{r} (\rho ,\theta) = \left(\rho \cos \theta ,\rho \sin \theta ,\frac{2}{3}\rho  \right).
\end{equation}

Therefore an infinitesimal surface area can be found by 

\begin{equation}
	\begin{aligned} 
	dS &= \left| \frac{\partial \vb{r} }{\partial \rho } _{} \cross  \frac{\partial \vb{r} }{\partial \theta }_{} \right| d\rho d\theta  = \begin{pmatrix}
		 \cos \theta  \\
		 \sin \theta  \\
		 2/3  \\
	\end{pmatrix} \cross \begin{pmatrix}
		 -\rho \sin \theta  \\
		 \rho \cos \theta  \\
		 0 \\
	\end{pmatrix} d \rho d \theta \\
	&= \left| \begin{pmatrix}
		 2\rho \cos  \theta /3  \\
		 2\rho \sin \theta /3  \\
		 \rho  \\
	\end{pmatrix} \right| d \rho d \theta = \frac{\sqrt{13} }{3} \rho d \rho d \theta .  
	\end{aligned} 
\end{equation}

The total surface area is therefore 

\begin{equation}
	S = \int_{S}^{} dS = \int_{0}^{2\pi } \int_{0}^{\frac{3}{2} } \frac{4\sqrt{13} }{3} \rho d \rho d \theta = \frac{3\pi }{4} \sqrt{13}.    
\end{equation}
} 

\example{Surface Integral (7).}
{Evaluate 

\begin{equation}
	\int_{S}^{} (x-z) dS,  
\end{equation}

where \(S\) is the surface of the solid bounded by \(x^2+y^2=4, z = x -3 \text { and } z = x+2\).  
}
{For the side surface, we use cylindrical coordinates 

\begin{equation}
	x = 2 \cos \theta , \quad y = 2 \sin \theta , \quad z \in [z_1 ,z_2 ], \quad z_1 = 2 \cos \theta - 3 ~\text { and }~ z_2 = 2 \cos \theta +2,
\end{equation}

where the limit in \(z\) is bound by the inequality \(z \ge x-3 \text { and } z \le x+2\).

For the bottom and the top cap we have \(dS = \abs{\grad{f(x,y,z)} } dxdy / (\partial f (x,y,z)  / \partial z) \), where \(f(x,y,z) = x-z+3\), so the whole surface integral becomes

\begin{equation}
	I = \int_{0}^{2\pi } \int_{2 \cos \theta -3}^{2\cos \theta +2} (2 \cos \theta -z) 2dz d \theta + \int 3 \sqrt{2}dxdy + \int -2 \sqrt{2} dxdy = \pi (10+4\sqrt{2} ).        
\end{equation}
~
} 



\subsection{Vector Areas}

The vector area of a surface \(S\) is defined as

\begin{equation}
	\vb{S}  = \int_{S}^{} d\vb{S} = \oint_{S} \vu{n} dS
\end{equation}

A closed surface will always has a zero vector area, since when projecting onto the \(xy\)-plane, from \cref{dseq} we see that every \( d\vb{S} _{+}  = dA/\abs{\vu{n} \cdot \vu{k} }\) will have an opposite contribution \( d\vb{S} _{-} = dA/\abs{- \vu{n} \cdot \vu{k} } = -d\vb{S} _{+} \). 

This fact implies that the vector area of any open surface \(S\) only depends on its perimeter curve \(C\), since we can construct a closed surface with arbitrary upper and lower surface and their contribution must be equals in magnitude so that the sum is zero. We know that their directions are the same due to right hand rule. 

Specifically, the vector area can be represented by the line integral 

\begin{equation}
	\vb{S} = \frac{1}{2} \oint_{C} \vb{r} \cross d\vb{r}, 
\end{equation}

since one of the possible surface spanned by the perimeter \(C\) is a cone with its vertex at the origin with the perimeter of the base \(C\) as shown in \cref{cone} and its area is the sum of all the infinitesimal triangle, each with vector area \( d\vb{S} =  \vb{r} \cross d\vb{r} /2  \).  

\onefig{cone}{scale=0.3}

For a surface confined to the \(xy\)-plane, \(\vb{r} = x \vu{i} + y \vu{j} \text { and } d\vb{r} = dx \vu{i} + dy \vu{j}\), thus \(\vb{r} \cross d\vb{r} = (xdy-ydx)\vu{k}\), so the area is what we have found earlier in \cref{area}.   


\subsection{Solid Angle}

The solid angle \(\Omega \) subtended at a point \(O\) by a surface \(S\) is defined as 

\begin{equation}
	\Omega = \int_{S}^{} \frac{\vu{r} \cdot d\vb{S} }{r^2} . 
\end{equation}

In particular, when the surface is clossed \(\Omega = 0\) if \(O\) is outside \(S\) and \(\Omega = 4\pi \) if \(O\) is an interior point.  

\section{Volume Integrals}

Since \(dV\) is a scalar, there are only two kinds of volume integrals

\begin{equation}
	\int_{V}^{} f(x,y,z)dV \text { and } \int_{V}^{} \vb{F} (x,y,z) dV.    
\end{equation}


Similar to how the vector area of a surface \(S\) can be represented by a line integral along its perimeter \(C\), the volume of a volume \(V\)  can be represented by a surface integral over the surface \(S\) that bounds it.

Referring to \cref{cone2}, we have

\begin{equation}
	V = \int_{V}^{} dV = \frac{1}{3} \oint_{S} \vb{r} \cdot d\vb{S}, 
\end{equation}

as the volume of each cone is \(dV =  \vb{r} \cdot d\vb{S} /3 \). 

\onefig{cone2}{scale=0.3} 

\example{Volume Integrals (1).}
{Find the volume of a sphere with a cylinder of raidus \(s < R\) removed from the middle. }
{The most natural choice of coordinates is the cylindrical corrdinates, and the region we want to integrate over is \(x^2+y^2+z^2 \le R^2\), together with \(x^2+y^2 \ge s ^2\). In cylindrical corrdinates, these become 

\begin{equation}
	s \le \rho \le R ~\text { and }~ -\sqrt{R^2-\rho ^2} \le z \le \sqrt{R^2-\rho ^2}. 
\end{equation}

So the volume is

\begin{equation}
	V = \int_{-\sqrt{R^2-\rho ^2} }^{\sqrt{R^2-\rho ^2} } \int_{0}^{2\pi } \int_{s}^{R} \rho d\rho d \phi dz = \frac{4\pi }{3} (R^2-s ^2)^{\frac{3}{2} }.       
\end{equation}

As we have noted, sometimes it is easier to find the limits of integration by writing them in Cartesian coordinates first and then express them in the coordinate system one is using.
} 


\section{Integral Theorems}

\subsection{The Divergence Theorem}
The divergence theorem states that

\begin{equation} \label{divthmm} 
	\int _{V}(\div{\vb{F} } )dV = \oint_{S} \vb{F} \cdot d\vb{S} .
\end{equation}

The corresponding 2D-version is 

\begin{equation}
	\int_{S}^{} (\div{\vb{F} } ) dS = \oint_{C} \vb{F} \cdot d\vb{r} . 
\end{equation}

With this pattern we can easily generalize the divergence theorem into higher dimensions. 

\example{Surface Integral by the Divergence Theorem.}
{Evaluate the surface integral \(I = \int_{S}^{} \vb{F} \cdot d\vb{S}  \), where \(\vb{F} = (y-x)\vu{i} + x^2z \vu{j} + (z+x^2) \vu{k}\) and \(S\) is the open surface of the hemisphere \(x^2 + y^2 + z^2 = a^2, z \ge 0\).}
{Consider the closed surface \(S' = S + S_1 \), where \(S_1 \) is the circular area in the \(xy\)-plane given by \(x^2 + y^2 \le a^2, z=0\). By the divergence theorem we have

\begin{equation}
	\int_{V}^{} (\div{\vb{F} } )dV = 0 = \int_{S}^{} \vb{F} \cdot d\vb{S} + \int_{S_1 }^{} \vb{F} \cdot d\vb{S}_{1} .  
\end{equation}

Therefore we can simly evaluate the surface integral over \(S_1 \) and add a negative sign to get the desired result. Thus

\begin{equation}
	I = - \int_{S_1 }^{} \vb{F} \cdot d\vb{S} _{1} = \int \int_{R}^{} x^2dxdy = \frac{\pi a^4}{4}.   
\end{equation}
} 

\example{Divergence Theorem.}
{Show that for any closed surface \(S\) in a vector field \(\vb{F} \) we have

\begin{equation}
	\oint_{S}  (\curl{\vb{F} } )\cdot d\vb{S} = 0. 
\end{equation}
~
}
{We apply the divergence theorem to the vector field \(\curl{\vb{F} } \) and we get directly

\begin{equation}
	\oint_{S} (\curl{\vb{F} } )\cdot d\vb{S} = \int_{V}^{} (\div{(\curl{\vb{F} } )} ) dV = 0. 
\end{equation}
~
} 


\example{The Continuity Equation.}
{For a compressible fluid with time-varing position-dependent density \(\rho (\vb{r}, t)\) and velocity field \(v(\vb{r}, t)\), in which fluid is neither being created nor destroyed, show that 

\begin{equation}
	\frac{\partial \rho }{\partial t} + \div{(\rho \vb{v} )} = 0.
\end{equation}~
}
{Consider an arbitrary volume \(V\) in the fluid bounded by \(S\). From conservation of mass, we have

\begin{equation}
	\frac{dM}{dt} = \frac{d}{dt} \int_{V}^{} \rho dV = - \oint_{S} \rho \vb{v} \cdot d\vb{S} 
\end{equation}

\begin{equation}
	\int_{V}^{} \frac{\partial \rho }{\partial t}dV + \int_{V}^{} \div{(\rho \vb{v} )} dV = \int_{V}^{} \left( \frac{\partial \rho }{\partial t} + \div{(\rho \vb{v} )}  \right) dV = 0.    
\end{equation}

But since the volume \(V\) is arbitrary the integrand must be identically zero, arriving at the desired result.

For the flow of an incompressible fluid \(\rho = \text{constant} \) and the continuity equation becomes simply \(\div{v} = 0\).~
}

\example{Predator-Prey Systems.}
{Suppose the population of the predator and prey are \(w(t) \text { and } c(t)\) respectively, then the set of differential equations governing this sytem is 

\begin{equation}
	\begin{cases}
		\frac{dw}{dt}  &= w(-\alpha +c-\mu w ),\\
		\frac{dc}{dt} &= c(\beta -w - \nu c ),
	\end{cases}
\end{equation}

where the meaning of the coefficients \((\alpha , \beta , \mu , \nu )\) can be easily interpreted.  

Does there exist a periodic orbit in the \(c\)-\(w\) phase space? 
}
{We first change the form of the differential equation as 

\begin{equation}
	\frac{d\vb{a} }{dt} = \vb{p} , \quad \vb{a} = \begin{pmatrix}
		 w \\
		 c \\
	\end{pmatrix}, \quad \vb{p} = \begin{pmatrix}
		 w(-\alpha +c-\mu w) \\
		 c(\beta -w-\nu c) \\
	\end{pmatrix},
\end{equation}

which makes it clear that \(\vb{p} \) is the tangent to any path \(\vb{a} (t)\) in the animal phase plane. Using the divergence theorem in 2 dimensions, we have

\begin{equation}
	\int_{S}^{} (\div{(b(w,c))\vb{p} } )dS = \oint_{C} b(w,c) \vb{p} \cdot \vu{n} dt,
\end{equation}

where \(b(c,w)\) can be any function of our choice. If \(b(w,c) = 1/wc\), then we have 

\begin{equation}
	\div{\left( \frac{\vb{p} }{wc}  \right)} = - \frac{\mu }{c} - \frac{\nu }{w},  
\end{equation}

which is strictly negative. However, if \(\div{(\vb{p} /wc)} \) is always negative then there's no way to integrate it over a region and get zero. There for by contradiction we conclude that there is no closed orbit in the animal phse plane. In other words, therei is no nice periodic solutions.


} 

\example{Integral Form and Differntial Form of Gauss's Law.}
{Asuuming spherical symmetry, prove that the integral form and the differential form of the Gauss's law are equivalent by considering the volume enclosed by two spherical shells of radius \(r \text { and } r+ dr\).   }
{Since there is spherical symmetry, we have \(\vb{E} (\vb{r} ) = \vb{E} (r)\). From the integral from of the Gauss's law we have

\begin{equation}
	\begin{aligned} 
	4\pi (r+dr)^2 E(r+dr) - 4\pi r^2 E(r) &= \frac{\rho (4\pi r^2dr)}{\epsilon_0 }\\
	(r^2+2rdr)(E(r)+dE) - r^2E(r) &= \frac{\rho r^2dr}{\epsilon_0 }\\
	2rE(r)dr +r^2dE &= \frac{\rho r^2dr}{\epsilon_0 }\\
	\frac{2E}{r} + \frac{dE}{dr} &= \frac{1}{r^2} \frac{d}{dr}(r^2E(r)) = \frac{\rho }{\epsilon_0 }\\     
	\end{aligned} 
\end{equation}

The differential form of the Gauss's law, on the other hand, reads

\begin{equation}
	\div{\vb{E} } = \frac{1}{r^2} \frac{d}{dr} ( r^2E(r)) = \frac{\rho }{\epsilon_0 },    
\end{equation}

which is equivalent to the integral form.


} 


\subsubsection{Green's Theorems}

Consider two scalar functions \(\phi \text { and } \psi \) in some volume \(V\) bounded by a surface \(S\). Applying the divergence theorem to the vector field \(\phi \grad{\psi } \), we get 

\begin{equation}
	\oint_{S} \phi \grad{\psi } \cdot d\vb{S} = \int_{V}^{} \div{(\phi \grad{\psi } )}dV = \int_{V}^{} (\phi \laplacian \psi +(\grad{\phi } )\cdot (\grad{\psi } ))dV.   
\end{equation}

This is known as the Green's first theorem. 

Reversing the roles of \(\phi \text { and } \psi \) in the above equation and subtracting the two equations gives

\begin{equation}
	\oint_{S} (\phi \grad{\psi } - \psi \grad{\phi } ) \cdot d\vb{S}  = \int_{V}^{} (\phi \laplacian \psi - \psi \laplacian \phi ) dV. 
\end{equation}

This is known as the Green's second theorem.

\subsubsection{Two Other Theorems}

Letting \(\vb{F} \) in \cref{divthmm} to be a gradient of another scalar function, or the cross product of two other vector functions, we get 

\begin{equation}
	\int_{V}^{} \grad{f} dV = \oint_{S} fd\vb{S} \text { and } \int_{V}^{} (\curl{\vb{F} }) dV = \oint_{S} d\vb{S} \cross \vb{F} .     
\end{equation}

\subsection{Stokes' Theorem}
The Stokes' theorem states that 

\begin{equation} \label{stothmm} 
	\int_{S}^{} (\curl{\vb{F} } ) \cdot d\vb{S}  = \oint_{C} \vb{F} \cdot d\vb{r} .
\end{equation}

In two dimensions, curl is not defiend, but the main idea stands:

\begin{equation}
	\int_{S}^{} \left( \frac{\partial F_{y} }{\partial x} - \frac{\partial F_{x} }{\partial y}  \right) dS = \oint_{C} (F_{x} dx+F_{y} dy). 
\end{equation}

This is known as the Green's theorem in a plane, which is equivalent to the two-dimensional divergence theorem. Apply to a rectangular region, Green's theorem in a plane reduces to the fundamental theorem of calculus.

A surface without holes is called simply-connected region, defined by the property that any closed path on the region can be contracted to a point without leaving the surface. In layman's term, it has more than one boundaries, or more sipmly, it has holes.

A surface that is orientable is one on which you can choose a continuous normal vector everywhere so that as you move around the surface, the normal never flips discontinuously. More simply put, you can distinguish its two-sides globally. A Möbius strip or a Klein bottle are exapmles of non-orientable surfaces. If you start with a chosen nomral vector at one point and carry it continuously around certain loops, you return to the original point with the normal reversed.

The Stokes' theorem only holds for smooth orientable surfaces. If the region is not sipmly-connected, then the integral should be done in an anti-clockwise direction around the exterior boundary, and in a clockwise direction on any interior boundary. The quickest way to see this is to do the integration around a continouous boundary as shown in \cref{green}, with an infinitesimal gap. The two contributions across the gap then cancel. The same technique can be used to apply Stokes' theorem to closed surface.



\onefig{green}{scale=0.3} 

\example{Stokes' Theorem (1).}
{Let \(S\) be the cap of a sphere of radius \(R\) that is covered by the angle \(0 \le \theta \le \alpha \). Confirm the Stokes' theorem with the vector field \(\vb{F} = (0,xz,0)\)  }
{The surface integral is 

\begin{equation}
	\int_{S}^{} (\curl{\vb{F} } ) \cdot d\vb{S}  = \int_{0}^{2\pi } \int_{0}^{\alpha } R^2\sin \theta d \theta d\phi \begin{pmatrix}
		 -x \\
		 0 \\
		 z \\
	\end{pmatrix} \cdot  \begin{pmatrix}
		 \sin \theta \cos \phi  \\
		 \sin \theta \sin \phi  \\
		 \cos \theta  \\
	\end{pmatrix} = \pi R^3 \cos \alpha \sin ^2 \alpha .    
\end{equation}

The line integral is performed around the rim \(C\) parameterized by the angle \(\phi \), given by 

\begin{equation}
	\vb{r} (\phi ) = R(\sin \alpha \cos \phi ,\sin \alpha \sin \phi ,\cos \alpha ) \implies d\vb{r} = R(-\sin \alpha \sin \phi , \sin \alpha \cos \phi , 0)d\phi.
\end{equation}

So the line integral is

\begin{equation}
	\oint_{C} \vb{F} \cdot d\vb{r} = \int_{0}^{2\pi } Rxz \sin \alpha \cos \phi d\phi = \pi R^3 \sin ^2\alpha \cos \alpha .  
\end{equation}

} 

\example{Stokes' Theorem (2).}
{Consider the conical surface \(S\) defined by \(z^2 = x^2+ y^2\) with \(0 < a \le z \le b\). Confirm the Stoke's theorem with the vector field \(\vb{F} = (0,xz,0)\).   }
{The surface is parameterized, in cylindrical polar coordinates, by 

\begin{equation}
	\vb{r} (\rho , \phi ) = (\rho \cos \phi , \rho \sin \phi , z)
\end{equation}

with \(a \le \rho \le b\) and \(0 \le \phi  < 2\pi \).  

So the infinitesimal surface vector area \(d\vb{S} \) can be found by 

\begin{equation}
	d\vb{S}  = \frac{\partial \vb{r} }{\partial \rho } \cross \frac{\partial \vb{r} }{\partial \phi } = (- \cos \phi , - \sin \phi , 1 ) \rho d\rho d\phi .  
\end{equation}

The surface integral is 

\begin{equation}
	\int_{S}^{} (\curl{\vb{F} } ) \cdot d\vb{S} = \int_{0}^{2\pi } \int_{a}^{b} \begin{pmatrix}
		 -x \\
		 0 \\
		 z \\
	\end{pmatrix} \cdot \begin{pmatrix}
		 -\cos \phi  \\
		 -\sin \phi  \\
		 1 \\
	\end{pmatrix} \rho d\rho d\phi = \pi (b^3 -a^3 ).
\end{equation}

Now the line integral is performed around the two circumferences \(C_1 \text { and } C_2 \) with radius \(R = a \text { and } b\), which are parameterized by the angle \(\phi \), given by 

\begin{equation}
	\vb{r} (\phi ) = R(\cos \phi , \sin \phi , 1) \implies d\vb{x} = R(-\sin \phi , \cos \phi ,0)d\phi ,
\end{equation}

so the line integral of one of the circumference is 

\begin{equation}
	\oint_{C} \vb{F} \cdot d\vb{r} = \int_{0}^{2\pi } R^3 \cos ^2\phi d\phi = \pi R^3 .  
\end{equation}

Since the orientation of \(C_1 \text { and } C_2 \) are in opposite direction, substituting \(R = a \text { and } b\) and subtracting the results give the same result as the surface integral.   
} 

\example{The Ampere's Law.}
{Convert the integral form of Ampere's Law into differential form}
{Amere's law for any circuit \(C\) bounding a surface \(S\) is given by 

\begin{equation}
	\oint_{C} \vb{B} \cdot d\vb{r} = \int_{S}^{} (\curl{\vb{B} } ) \cdot d\vb{S} =  \mu _{0} I = \mu _{0}  \int_{S}^{} \vb{J} \cdot d\vb{S} .  
\end{equation}

Hence 

\begin{equation}
	\int_{S}^{} (\curl{\vb{B} } - \mu _{0} \vb{J}  ) \cdot d\vb{S} = 0.
\end{equation}
}

\example{Area of an Ellipse.}
{Show that the area of a region \(R\) enclosed by a simple closed curve \(C\) is given by 

\begin{equation}
	A = \frac{1}{2} \oint_{C}(xdy - ydx) = \oint_{C}xdy = - \oint_{C}y dx.
\end{equation}

Hence calculate the area of the ellipse with the parameterization \(x = a \cos \phi , y = b\sin \phi \). }
{By Green's theorem we have 

\begin{equation} \label{area} 
	\oint_{C} (xdy- ydx) = \int_{S}^{}  (1+1) dxdy = 2A.  
\end{equation}

Similarly, 

\begin{equation}
	\oint_{C} xdy = \int_{S}^{} (1) dxdy = \oint_{C} -ydx = A. 
\end{equation}

Therefore the area of an ellipse is 

\begin{equation}
	A = \frac{1}{2} \int_{0}^{2\pi } ab(\cos ^2\phi + \sin ^2\phi ) d\phi  = \pi ab.   
\end{equation}
} 

\example{Green's Theorem in a Plane}
{A closed path \(C\) is described by the upper half \((y \ge 0)\) o f the circle of radius \(1\) in the \(x\)-\(y\) plane centered at the origin, and the portion of \(y = x^2-1\) between \(-1 \le x \le 1\). Evaluate the line integral 

\begin{equation}
	\oint_{C} \vb{F} \cdot d\vb{r} , \quad \vb{F} = 3y \vu{i} + (x^2-y)\vu{j} .
\end{equation}
~
}
{The upper semicircle is parameterized by the polar angle \(\theta \) as

\begin{equation}
	\vb{r}_{1}  (\theta ) = (\cos \theta ,\sin \theta ), \quad d\vb{r} _{1}(\theta ) = (-\sin \theta ,\cos \theta )d\theta.
\end{equation}

The integrand is therefore 

\begin{equation}
	\vb{F} (\vb{r} _{1}(\theta ) ) \cdot d\vb{r} _{1} = (3 \sin \theta , \cos ^2\theta -\sin \theta ) \cdot (-\sin \theta ,\cos \theta ) dt = -3 \sin ^2 \theta + \cos ^3 \theta -\sin \theta \cos \theta . 
\end{equation}

Integrating over \(\theta \) from \(0 \) to \(\pi \) gives \(I_1  = -3\pi /2\).

The lower parabola is parameterized by \(x\) since its algebraic form is given

\begin{equation}
	\vb{r} _{2}(x) = (x, x^2-1). 
\end{equation}

The integrand is therefore 

\begin{equation}
	\vb{F} (\vb{r} _{2}(x) ) \cdot d\vb{r} _{2} = (3(x^2-1), 1) \cdot (1,2x)dx .
\end{equation}

Integrating over \(x\) from \(-1\) to \(1\) gives \(I_2 = -4\). 

So the final answer is \(I = I_1 +I_2 = -3 \pi /2 -4\). 

A much faster method is to use the Green's theorem in a plane, which gives

\begin{equation}
	\begin{aligned} 
	\oint_{C} \vb{F} \cdot d\vb{r} &= \oint_{C} (3y \vu{i} + (x^2-y)\vu{j} ) \cdot (dx \vu{i} + dy \vu{j} ) = \oint_{C} (3ydx+(x^2-y)dy) \\
	&= \int_{S}^{} (2x-3)dS =   \int_{-1}^{1} \int_{x^2-1}^{\sqrt{1-x^2} } (2x-3)dydx = -\frac{3\pi }{2} - 4.    
	\end{aligned} 
\end{equation}
~
} 

\example{Stokes' Theorem of a Closed Surface.}
{Prove that for a closed surface \(S\) 

\begin{equation}
	\oint_{S} (\curl{\vb{F} } ) \cdot d\vb{S} = 0.
\end{equation}
~
}
{From the Stokes' theorem 

\begin{equation}
	\int_{S}^{} (\curl{\vb{F} } ) \cdot d\vb{S} = \oint_{C} \vb{F} \cdot d\vb{r} , 
\end{equation}

RHS is trivially zero since a closed surface has no boundary curve, so LHS is also zero.

From the divergence theorem

\begin{equation}
	\int_{V}^{} \div{(\curl{\vb{F} } )} dV = \oint_{S} (\curl{\vb{F} } ) \cdot d\vb{S} , 
\end{equation}

LHS is trivially zero due to the vector identity \(\div{(\curl{\vb{F} } )} = 0 \), so RHS is also zero. 
} 

The Green's theorem is also valid for region with holes, however, the line integral must be carry out in the direction that a person travelling along the boundaries always has the region \(R\) on their left.

We also see that if the line integral around a closed loop is zero, Green's theorem implies that \(\partial P /\partial y = \partial Q /\partial x\), which is equivalent to saying that \(P(x,y) dx + Q(x,y) dy \) is an exact differential such that it equals to the differential for some function \(\phi (x,y)\) and for a closed loop the beginning and the end points are the same thus we evaluate \(\phi \) at the same point and thus the result is zero.

\subsubsection{The Two Other Theorems}

Letting \(\vb{F} \) in \cref{stothmm} to be a gradient of another scalar function, or the cross product of two other vector functions, we get 

\begin{equation}
	\int_{S}^{} d\vb{S} \cross \grad{f} = \oint_{C} fd\vb{r} \text { and } \int_{S}^{} (d\vb{S} \cross \grad{} ) \cross \vb{F} = \oint_{C} d\vb{r} \cross \vb{F} .    
\end{equation}
















\begin{appendices}
\chapter{Rigorous Proofs}
\section{Leibnitz' Theorem} \label{leibnitzapp} 

Here we provide a proof for \cref{leibnitz}.

\begin{proof}
Suppose \cref{lei} is valid for \(n\) equals to some integer \(N\), then

\begin{equation}
    \begin{aligned}
        f^{(N+1)} &= \sum_{r=0}^{N} \binom{n}{r} \frac{d}{dx}(u^{(r)} v^{(N-r)} ) \\
        &= \sum_{r=0}^{N} \binom{N}{r} (u^{(r)} v^{(N-r+1)} + u^{(r+1)}v^{(N-r)}  ) \\
        &= \sum_{s=0}^{N} \binom{N}{s}u^{(s)}v^{(N+1-s)} + \sum_{s=1}^{N+1} \binom{N}{s-1} u^{(s)}v^{(N+1-s)} \\
        &= \binom{N}{0}u^{(0)} v^{(N+1)} + \sum_{s=1}^{N} \binom{N+1}{s}u^{(s)}v^{(N+1-s)} + \binom{N}{N}u^{(N+1)} v^{(0)} \\
        &= \binom{N+1}{0}u^{(0)} v^{(N+1)} + \sum_{s=1}^{N} \binom{N+1}{s}u^{(s)}v^{(N+1-s)} + \binom{N+1}{N+1}u^{(N+1)} v^{(0)} \\
        &= \sum_{s=0}^{N+1} \binom{N+1}{s} u^{(s)}v^{(N+1-s)}.  
    \end{aligned}
\end{equation}

Since \(N=1\) corresponds to prodouct rule, which is trivial, by induction we have proved \cref{lei} holds for all positive integers \(n\). 

\end{proof}


\end{appendices}
\end{document}